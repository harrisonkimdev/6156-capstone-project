{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BetaMove Model Training on Google Colab\n",
    "\n",
    "This notebook enables training YOLO and XGBoost models using Google Colab's GPU resources.\n",
    "\n",
    "## Workflow\n",
    "1. Mount Google Drive (training data storage)\n",
    "2. Clone the repository\n",
    "3. Install dependencies\n",
    "4. Load training data from Google Drive\n",
    "5. Train the model\n",
    "6. Save results to Google Drive\n",
    "\n",
    "## Prerequisites\n",
    "- Training data uploaded to your Google Drive\n",
    "- GPU runtime enabled (Runtime > Change runtime type > GPU)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Mount Google Drive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "# Configure paths - adjust these to your setup\n",
    "DRIVE_ROOT = '/content/drive/MyDrive'\n",
    "BETAMOVE_DIR = f'{DRIVE_ROOT}/BetaMove'  # Root folder for BetaMove data\n",
    "TRAINING_DATA_DIR = f'{BETAMOVE_DIR}/training_data'\n",
    "MODELS_DIR = f'{BETAMOVE_DIR}/models'\n",
    "\n",
    "print(f'Google Drive mounted at: {DRIVE_ROOT}')\n",
    "print(f'Training data directory: {TRAINING_DATA_DIR}')\n",
    "print(f'Models directory: {MODELS_DIR}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Clone Repository and Install Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "# Clone the repository\n",
    "REPO_URL = 'https://github.com/harrisonkimdev/6156-capstone-project.git'\n",
    "REPO_DIR = '/content/6156-capstone-project'\n",
    "\n",
    "if not Path(REPO_DIR).exists():\n",
    "    !git clone {REPO_URL}\n",
    "    print(f'Repository cloned to {REPO_DIR}')\n",
    "else:\n",
    "    %cd {REPO_DIR}\n",
    "    !git pull\n",
    "    print('Repository updated')\n",
    "\n",
    "%cd {REPO_DIR}\n",
    "print(f'Working directory: {os.getcwd()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install dependencies\n",
    "!pip install -q -r requirements.txt\n",
    "\n",
    "# Add src to Python path\n",
    "sys.path.insert(0, str(Path(REPO_DIR) / 'src'))\n",
    "\n",
    "print('Dependencies installed')\n",
    "print(f'Python path includes: {Path(REPO_DIR) / \"src\"}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Check GPU Availability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    device = 'cuda'\n",
    "    gpu_name = torch.cuda.get_device_name(0)\n",
    "    gpu_memory = torch.cuda.get_device_properties(0).total_memory / 1e9\n",
    "    print(f'GPU available: {gpu_name}')\n",
    "    print(f'GPU memory: {gpu_memory:.1f} GB')\n",
    "else:\n",
    "    device = 'cpu'\n",
    "    print('WARNING: No GPU available. Training will be slow.')\n",
    "    print('Go to Runtime > Change runtime type > GPU')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Setup Training Data Directories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "# Create directories if they don't exist\n",
    "for dir_path in [TRAINING_DATA_DIR, MODELS_DIR]:\n",
    "    Path(dir_path).mkdir(parents=True, exist_ok=True)\n",
    "    print(f'Directory ready: {dir_path}')\n",
    "\n",
    "# List existing training data\n",
    "print('\\nExisting training data:')\n",
    "if Path(TRAINING_DATA_DIR).exists():\n",
    "    for item in Path(TRAINING_DATA_DIR).iterdir():\n",
    "        if item.is_dir():\n",
    "            files = list(item.rglob('*'))\n",
    "            print(f'  {item.name}/ ({len(files)} files)')\n",
    "        else:\n",
    "            print(f'  {item.name} ({item.stat().st_size / 1024:.1f} KB)')\n",
    "else:\n",
    "    print('  (empty - upload training data to Google Drive)')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# YOLO Hold Detection Training\n",
    "\n",
    "Train a YOLO model for hold detection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration for YOLO training\n",
    "YOLO_CONFIG = {\n",
    "    'dataset_yaml': f'{TRAINING_DATA_DIR}/hold_detection/dataset.yaml',  # Path to dataset.yaml in Drive\n",
    "    'model': 'yolov8n.pt',  # Base model: yolov8n.pt, yolov8s.pt, yolov8m.pt\n",
    "    'epochs': 100,\n",
    "    'batch': 16,\n",
    "    'imgsz': 640,\n",
    "    'project': f'{MODELS_DIR}/yolo_runs',\n",
    "    'name': 'hold_detection',\n",
    "}\n",
    "\n",
    "print('YOLO Training Configuration:')\n",
    "for key, value in YOLO_CONFIG.items():\n",
    "    print(f'  {key}: {value}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify dataset exists\n",
    "dataset_yaml = Path(YOLO_CONFIG['dataset_yaml'])\n",
    "if not dataset_yaml.exists():\n",
    "    print(f'ERROR: Dataset not found at {dataset_yaml}')\n",
    "    print('\\nPlease upload your YOLO dataset to Google Drive with this structure:')\n",
    "    print(f'  {TRAINING_DATA_DIR}/hold_detection/')\n",
    "    print('    ├── dataset.yaml')\n",
    "    print('    ├── images/')\n",
    "    print('    │   ├── train/')\n",
    "    print('    │   └── val/')\n",
    "    print('    └── labels/')\n",
    "    print('        ├── train/')\n",
    "    print('        └── val/')\n",
    "else:\n",
    "    print(f'Dataset found: {dataset_yaml}')\n",
    "    print('\\nDataset configuration:')\n",
    "    print(dataset_yaml.read_text())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train YOLO model\n",
    "from ultralytics import YOLO\n",
    "from datetime import datetime\n",
    "import json\n",
    "\n",
    "print('Starting YOLO training...')\n",
    "print(f'Time: {datetime.now().isoformat()}')\n",
    "\n",
    "# Load model\n",
    "model = YOLO(YOLO_CONFIG['model'])\n",
    "\n",
    "# Train\n",
    "results = model.train(\n",
    "    data=YOLO_CONFIG['dataset_yaml'],\n",
    "    epochs=YOLO_CONFIG['epochs'],\n",
    "    batch=YOLO_CONFIG['batch'],\n",
    "    imgsz=YOLO_CONFIG['imgsz'],\n",
    "    project=YOLO_CONFIG['project'],\n",
    "    name=YOLO_CONFIG['name'],\n",
    "    device=device,\n",
    "    verbose=True,\n",
    ")\n",
    "\n",
    "print(f'\\nTraining completed at: {datetime.now().isoformat()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save training metadata\n",
    "import json\n",
    "from datetime import datetime\n",
    "\n",
    "# Get best model path\n",
    "best_model_path = Path(YOLO_CONFIG['project']) / YOLO_CONFIG['name'] / 'weights' / 'best.pt'\n",
    "\n",
    "# Extract metrics\n",
    "metrics = {}\n",
    "if hasattr(results, 'results_dict'):\n",
    "    metrics = results.results_dict\n",
    "\n",
    "# Create metadata\n",
    "metadata = {\n",
    "    'model_type': 'yolo',\n",
    "    'base_model': YOLO_CONFIG['model'],\n",
    "    'hyperparameters': {\n",
    "        'epochs': YOLO_CONFIG['epochs'],\n",
    "        'batch': YOLO_CONFIG['batch'],\n",
    "        'imgsz': YOLO_CONFIG['imgsz'],\n",
    "    },\n",
    "    'metrics': metrics,\n",
    "    'best_model_path': str(best_model_path),\n",
    "    'trained_on': 'colab',\n",
    "    'device': device,\n",
    "    'completed_at': datetime.now().isoformat(),\n",
    "}\n",
    "\n",
    "# Save metadata\n",
    "metadata_path = best_model_path.parent.parent / 'model_metadata.json'\n",
    "with open(metadata_path, 'w') as f:\n",
    "    json.dump(metadata, f, indent=2, default=str)\n",
    "\n",
    "print(f'Metadata saved to: {metadata_path}')\n",
    "print(f'Best model saved to: {best_model_path}')\n",
    "print('\\nTraining Results:')\n",
    "print(json.dumps(metadata, indent=2, default=str))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# XGBoost Pose Classification Training\n",
    "\n",
    "Train an XGBoost model for pose classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration for XGBoost training\n",
    "XGB_CONFIG = {\n",
    "    'features_path': f'{TRAINING_DATA_DIR}/pose_features/features.json',  # Path to features file\n",
    "    'task': 'classification',\n",
    "    'label_column': 'detection_score',\n",
    "    'label_threshold': 0.6,\n",
    "    'test_size': 0.2,\n",
    "    'n_estimators': 300,\n",
    "    'learning_rate': 0.05,\n",
    "    'max_depth': 4,\n",
    "    'model_out': f'{MODELS_DIR}/xgb_pose/model.json',\n",
    "}\n",
    "\n",
    "print('XGBoost Training Configuration:')\n",
    "for key, value in XGB_CONFIG.items():\n",
    "    print(f'  {key}: {value}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify features file exists\n",
    "features_path = Path(XGB_CONFIG['features_path'])\n",
    "if not features_path.exists():\n",
    "    print(f'ERROR: Features file not found at {features_path}')\n",
    "    print('\\nPlease upload your pose features to Google Drive:')\n",
    "    print(f'  {TRAINING_DATA_DIR}/pose_features/features.json')\n",
    "else:\n",
    "    import json\n",
    "    with open(features_path) as f:\n",
    "        data = json.load(f)\n",
    "    print(f'Features file found: {features_path}')\n",
    "    print(f'Number of samples: {len(data)}')\n",
    "    if data:\n",
    "        print(f'Features per sample: {len(data[0].keys())}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train XGBoost model\n",
    "from pose_ai.ml.xgb_trainer import TrainParams, train_from_file\n",
    "from datetime import datetime\n",
    "import json\n",
    "\n",
    "print('Starting XGBoost training...')\n",
    "print(f'Time: {datetime.now().isoformat()}')\n",
    "\n",
    "# Create output directory\n",
    "model_out = Path(XGB_CONFIG['model_out'])\n",
    "model_out.parent.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Create training parameters\n",
    "params = TrainParams(\n",
    "    task=XGB_CONFIG['task'],\n",
    "    label_column=XGB_CONFIG['label_column'],\n",
    "    label_threshold=XGB_CONFIG['label_threshold'],\n",
    "    test_size=XGB_CONFIG['test_size'],\n",
    "    n_estimators=XGB_CONFIG['n_estimators'],\n",
    "    learning_rate=XGB_CONFIG['learning_rate'],\n",
    "    max_depth=XGB_CONFIG['max_depth'],\n",
    "    model_out=model_out,\n",
    ")\n",
    "\n",
    "# Train\n",
    "metrics = train_from_file(features_path, params)\n",
    "\n",
    "print(f'\\nTraining completed at: {datetime.now().isoformat()}')\n",
    "print(f'Metrics: {metrics}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save XGBoost training metadata\n",
    "import json\n",
    "from datetime import datetime\n",
    "\n",
    "metadata = {\n",
    "    'model_type': 'xgboost',\n",
    "    'hyperparameters': {\n",
    "        'task': XGB_CONFIG['task'],\n",
    "        'n_estimators': XGB_CONFIG['n_estimators'],\n",
    "        'learning_rate': XGB_CONFIG['learning_rate'],\n",
    "        'max_depth': XGB_CONFIG['max_depth'],\n",
    "        'test_size': XGB_CONFIG['test_size'],\n",
    "    },\n",
    "    'metrics': metrics,\n",
    "    'model_path': str(model_out),\n",
    "    'features_path': str(features_path),\n",
    "    'trained_on': 'colab',\n",
    "    'completed_at': datetime.now().isoformat(),\n",
    "}\n",
    "\n",
    "# Save metadata\n",
    "metadata_path = model_out.parent / 'model_metadata.json'\n",
    "with open(metadata_path, 'w') as f:\n",
    "    json.dump(metadata, f, indent=2, default=str)\n",
    "\n",
    "print(f'Metadata saved to: {metadata_path}')\n",
    "print(f'Model saved to: {model_out}')\n",
    "print('\\nTraining Results:')\n",
    "print(json.dumps(metadata, indent=2, default=str))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Summary\n",
    "\n",
    "After training, your models are saved to Google Drive at:\n",
    "- YOLO: `{MODELS_DIR}/yolo_runs/hold_detection/weights/best.pt`\n",
    "- XGBoost: `{MODELS_DIR}/xgb_pose/model.json`\n",
    "\n",
    "To use these models in the BetaMove application:\n",
    "1. Download the model files from Google Drive\n",
    "2. Place them in the appropriate `models/` directory\n",
    "3. Update the model paths in the application configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List all trained models in Google Drive\n",
    "print('Trained models in Google Drive:')\n",
    "print('=' * 50)\n",
    "\n",
    "models_path = Path(MODELS_DIR)\n",
    "if models_path.exists():\n",
    "    for model_dir in models_path.iterdir():\n",
    "        if model_dir.is_dir():\n",
    "            print(f'\\n{model_dir.name}/')\n",
    "            for item in model_dir.rglob('*'):\n",
    "                if item.is_file():\n",
    "                    rel_path = item.relative_to(model_dir)\n",
    "                    size_kb = item.stat().st_size / 1024\n",
    "                    print(f'  {rel_path} ({size_kb:.1f} KB)')\n",
    "else:\n",
    "    print('No models found')"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
