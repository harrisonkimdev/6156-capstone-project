{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fcf437c3",
   "metadata": {},
   "source": [
    "# 6156 Capstone Project\n",
    "\n",
    "Group 5: Connor Lynch, Harrison Kim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1c4abe35",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fatal: destination path '6156-capstone-project' already exists and is not an empty directory.\n",
      "/Users/harrisonkim/code/repos/6156-capstone-project/notebooks/6156-capstone-project\n",
      "Cloned Repository!\n",
      "Current location: /Users/harrisonkim/code/repos/6156-capstone-project/notebooks/6156-capstone-project\n"
     ]
    }
   ],
   "source": [
    "# Clone directly from GitHub\n",
    "!git clone https://github.com/harrisonkimdev/6156-capstone-project.git\n",
    "%cd 6156-capstone-project\n",
    "\n",
    "import os\n",
    "print(\"Cloned Repository!\")\n",
    "print(f\"Current location: {os.getcwd()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f943e02",
   "metadata": {},
   "source": [
    "## Install required packages and configure pose_ai module path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e0969212",
   "metadata": {
    "tags": [
     "setup"
    ]
   },
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "unterminated string literal (detected at line 79) (498671747.py, line 79)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Cell \u001b[0;32mIn[3], line 79\u001b[0;36m\u001b[0m\n\u001b[0;31m    Current working directory: {Path.cwd()}\")\u001b[0m\n\u001b[0m                                           ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m unterminated string literal (detected at line 79)\n"
     ]
    }
   ],
   "source": [
    "# 1. Install packages (protobuf>=5.29.1 avoids Colab resolver conflicts)\n",
    "import os\n",
    "import sys\n",
    "import subprocess\n",
    "from pathlib import Path\n",
    "\n",
    "def find_repo_root() -> Path:\n",
    "    candidates = [\n",
    "        Path.cwd(),\n",
    "        Path.cwd() / '6156-capstone-project',\n",
    "        Path('/content/6156-capstone-project'),\n",
    "    ]\n",
    "    candidates.extend(Path.cwd().parents)\n",
    "    seen = set()\n",
    "    for candidate in candidates:\n",
    "        candidate = candidate.resolve()\n",
    "        if candidate in seen:\n",
    "            continue\n",
    "        seen.add(candidate)\n",
    "        if (candidate / 'src').exists():\n",
    "            return candidate\n",
    "    raise FileNotFoundError('Could not locate repo root containing src/. Make sure the repo is cloned.')\n",
    "\n",
    "repo_root = find_repo_root()\n",
    "if Path.cwd().resolve() != repo_root:\n",
    "    os.chdir(repo_root)\n",
    "    print(f'Changed working directory to repo root: {repo_root}')\n",
    "else:\n",
    "    print(f'Using existing working directory: {repo_root}')\n",
    "\n",
    "requirements_path = repo_root / 'requirements.txt'\n",
    "fallback_packages = [\n",
    "    'numpy',\n",
    "    'pandas',\n",
    "    'scikit-learn',\n",
    "    'xgboost',\n",
    "    'opencv-python',\n",
    "    'mediapipe',\n",
    "    'protobuf>=5.29.1,<7',\n",
    "]\n",
    "\n",
    "def _pip_install(*args):\n",
    "    cmd = [sys.executable, '-m', 'pip', *args]\n",
    "    print('Running:', ' '.join(cmd))\n",
    "    subprocess.run(cmd, check=True)\n",
    "\n",
    "_pip_install('install', '-q', '--upgrade', 'pip')\n",
    "if requirements_path.exists():\n",
    "    _pip_install('install', '-q', '-r', str(requirements_path))\n",
    "else:\n",
    "    print(f'⚠️ requirements.txt not found at {requirements_path}, installing fallback packages.')\n",
    "    _pip_install('install', '-q', *fallback_packages)\n",
    "\n",
    "# 2. Configure path to import pose_ai module\n",
    "# Add src folder to Python path\n",
    "src_path = repo_root / 'src'\n",
    "\n",
    "if src_path.exists():\n",
    "    if str(src_path) not in sys.path:\n",
    "        sys.path.insert(0, str(src_path))\n",
    "    os.environ.setdefault('PYTHONPATH', str(src_path) + (':' + os.environ.get('PYTHONPATH', '')))\n",
    "    print(f\"✓ src path added: {src_path}\")\n",
    "else:\n",
    "    print(f\"⚠️ Warning: src folder not found: {src_path}\")\n",
    "    print('   Please ensure the repository is cloned or src/ folder exists.')\n",
    "\n",
    "# 3. Verify pose_ai import\n",
    "try:\n",
    "    import pose_ai as _pose_ai\n",
    "    print(f\"✓ pose_ai module imported successfully! (version: {getattr(_pose_ai, '__version__', 'unknown')})\")\n",
    "except Exception as e:  # pylint: disable=broad-exception-caught\n",
    "    print(f\"✗ pose_ai import failed: {e}\")\n",
    "    print('  Solutions:')\n",
    "    print('  1. Verify the repository is cloned correctly')\n",
    "    print('  2. Check if src/pose_ai/ folder exists')\n",
    "    print('  3. Refer to \"How to Use in Google Colab\" cell above for repo setup')\n",
    "\n",
    "# Display current working directory\n",
    "Current working directory: {Path.cwd()}\")\n",
    "print(f\"src added to Python path: {str(src_path) if src_path.exists() else 'N/A'}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scripts Python Collection\n",
    "\n",
    "This notebook aggregates the Python scripts from the `scripts` directory."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## scripts/extract_frames.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'pose_ai'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 9\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mlogging\u001b[39;00m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mpathlib\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Path\n\u001b[0;32m----> 9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mpose_ai\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdata\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m FrameExtractionResult, extract_frames_every_n_seconds, iter_video_files\n\u001b[1;32m     12\u001b[0m LOGGER \u001b[38;5;241m=\u001b[39m logging\u001b[38;5;241m.\u001b[39mgetLogger(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpose_ai.scripts.extract_frames\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     15\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mconfigure_logging\u001b[39m(verbose: \u001b[38;5;28mbool\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'pose_ai'"
     ]
    }
   ],
   "source": [
    "\"\"\"CLI for extracting frame sequences from climbing videos.\"\"\"\n",
    "\n",
    "from __future__ import annotations\n",
    "\n",
    "import argparse\n",
    "import logging\n",
    "from pathlib import Path\n",
    "\n",
    "from pose_ai.data import FrameExtractionResult, extract_frames_every_n_seconds, iter_video_files\n",
    "\n",
    "\n",
    "LOGGER = logging.getLogger(\"pose_ai.scripts.extract_frames\")\n",
    "\n",
    "\n",
    "def configure_logging(verbose: bool) -> None:\n",
    "    level = logging.DEBUG if verbose else logging.INFO\n",
    "    logging.basicConfig(\n",
    "        level=level,\n",
    "        format=\"%(asctime)s [%(levelname)s] %(name)s: %(message)s\",\n",
    "    )\n",
    "\n",
    "\n",
    "def extract_from_directory(\n",
    "    source_dir: Path,\n",
    "    *,\n",
    "    output_root: Path,\n",
    "    interval_seconds: float,\n",
    "    recursive: bool,\n",
    "    overwrite: bool,\n",
    "    write_manifest: bool,\n",
    ") -> list[FrameExtractionResult]:\n",
    "    results: list[FrameExtractionResult] = []\n",
    "    for video_path in iter_video_files(source_dir, recursive=recursive):\n",
    "        LOGGER.info(\"Processing %s\", video_path)\n",
    "        result = extract_frames_every_n_seconds(\n",
    "            video_path,\n",
    "            interval_seconds=interval_seconds,\n",
    "            output_root=output_root,\n",
    "            write_manifest=write_manifest,\n",
    "            overwrite=overwrite,\n",
    "        )\n",
    "        LOGGER.info(\n",
    "            \"Saved %d frames for %s\",\n",
    "            result.saved_frames,\n",
    "            video_path.name,\n",
    "        )\n",
    "        results.append(result)\n",
    "    if not results:\n",
    "        LOGGER.warning(\"No video files found in %s\", source_dir)\n",
    "    return results\n",
    "\n",
    "\n",
    "def build_parser() -> argparse.ArgumentParser:\n",
    "    parser = argparse.ArgumentParser(\n",
    "        description=\"Extract frame sequences from climbing videos.\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"video_dir\",\n",
    "        type=Path,\n",
    "        help=\"Directory containing source video files.\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--output\",\n",
    "        type=Path,\n",
    "        default=Path(\"data\") / \"frames\",\n",
    "        help=\"Directory where frame folders will be stored.\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--interval\",\n",
    "        type=float,\n",
    "        default=1.0,\n",
    "        help=\"Seconds between captured frames (default: 1.0).\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--recursive\",\n",
    "        action=\"store_true\",\n",
    "        help=\"Search for videos recursively.\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--overwrite\",\n",
    "        action=\"store_true\",\n",
    "        help=\"Overwrite existing extracted frames.\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--no-manifest\",\n",
    "        dest=\"write_manifest\",\n",
    "        action=\"store_false\",\n",
    "        help=\"Disable writing manifest.json files.\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--verbose\",\n",
    "        action=\"store_true\",\n",
    "        help=\"Enable verbose logging.\",\n",
    "    )\n",
    "    return parser\n",
    "\n",
    "\n",
    "def main() -> None:\n",
    "    parser = build_parser()\n",
    "    args = parser.parse_args()\n",
    "\n",
    "    configure_logging(verbose=args.verbose)\n",
    "    extract_from_directory(\n",
    "        args.video_dir,\n",
    "        output_root=args.output,\n",
    "        interval_seconds=args.interval,\n",
    "        recursive=args.recursive,\n",
    "        overwrite=args.overwrite,\n",
    "        write_manifest=args.write_manifest,\n",
    "    )\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## scripts/run_feature_export.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"CLI to export pose-derived features from manifests.\"\"\"\n",
    "\n",
    "from __future__ import annotations\n",
    "\n",
    "import argparse\n",
    "import json\n",
    "from pathlib import Path\n",
    "\n",
    "from pose_ai.service import export_features_for_manifest\n",
    "\n",
    "\n",
    "def build_parser() -> argparse.ArgumentParser:\n",
    "    parser = argparse.ArgumentParser(description=\"Export pose feature rows from pose_results.json.\")\n",
    "    parser.add_argument(\"manifest\", type=Path, help=\"Path to manifest.json\")\n",
    "    parser.add_argument(\n",
    "        \"--holds\",\n",
    "        type=Path,\n",
    "        help=\"Optional JSON describing holds (name -> coords, normalized, etc).\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--out\",\n",
    "        type=Path,\n",
    "        default=None,\n",
    "        help=\"Output directory (defaults to manifest directory).\",\n",
    "    )\n",
    "    return parser\n",
    "\n",
    "\n",
    "def main() -> None:\n",
    "    parser = build_parser()\n",
    "    args = parser.parse_args()\n",
    "    output_path = export_features_for_manifest(\n",
    "        args.manifest,\n",
    "        holds_path=args.holds,\n",
    "        output_root=args.out,\n",
    "    )\n",
    "    print(f\"Feature rows saved to {output_path}\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## scripts/run_pipeline.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"End-to-end pipeline: extract frames, run pose estimation, features, segments, visualize.\"\"\"\n",
    "\n",
    "from __future__ import annotations\n",
    "\n",
    "import argparse\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "ROOT_DIR = Path(__file__).resolve().parents[1]\n",
    "SRC_DIR = ROOT_DIR / \"src\"\n",
    "if str(SRC_DIR) not in sys.path:\n",
    "    sys.path.insert(0, str(SRC_DIR))\n",
    "\n",
    "from pose_ai.service import (  # type: ignore  # pylint: disable=wrong-import-position\n",
    "    estimate_poses_from_manifest,\n",
    "    export_features_for_manifest,\n",
    "    generate_segment_report,\n",
    ")\n",
    "from extract_frames import extract_frames_every_n_seconds, iter_video_files  # type: ignore\n",
    "from visualize_pose import visualize_pose_results  # type: ignore\n",
    "\n",
    "# NOTE: For simplicity we call into script helpers directly for pose/feature export\n",
    "# and reuse service APIs for intermediate steps.\n",
    "\n",
    "def build_parser() -> argparse.ArgumentParser:\n",
    "    parser = argparse.ArgumentParser(description=\"Run entire pose analysis pipeline\")\n",
    "    parser.add_argument(\"video_dir\", type=Path, help=\"Directory containing videos (.mp4, etc.)\")\n",
    "    parser.add_argument(\"--out\", type=Path, default=Path(\"data/frames\"), help=\"Frame output directory\")\n",
    "    parser.add_argument(\"--interval\", type=float, default=1.0, help=\"Extraction interval (seconds)\")\n",
    "    parser.add_argument(\"--skip-visuals\", action=\"store_true\", help=\"Skip visualization step\")\n",
    "    return parser\n",
    "\n",
    "\n",
    "def extract_frames(video_dir: Path, out_dir: Path, interval: float) -> list[Path]:\n",
    "    manifests = []\n",
    "    out_dir.mkdir(parents=True, exist_ok=True)\n",
    "    for video_file in iter_video_files(video_dir):\n",
    "        result = extract_frames_every_n_seconds(video_file, output_root=out_dir, interval_seconds=interval)\n",
    "        manifests.append(result.frame_directory / \"manifest.json\")\n",
    "    return manifests\n",
    "\n",
    "\n",
    "def main() -> None:\n",
    "    parser = build_parser()\n",
    "    args = parser.parse_args()\n",
    "    manifests = extract_frames(args.video_dir, args.out, args.interval)\n",
    "    for manifest in manifests:\n",
    "        print(f\"Processing manifest {manifest}\")\n",
    "        estimate_poses_from_manifest(manifest)\n",
    "        export_features_for_manifest(manifest)\n",
    "        generate_segment_report(manifest)\n",
    "        if not args.skip_visuals:\n",
    "            frame_dir = manifest.parent\n",
    "            visualize_pose_results(frame_dir / \"pose_results.json\")\n",
    "    print(\"Pipeline completed.\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## scripts/run_pose_estimation.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"CLI to run pose estimation on extracted frame sequences.\"\"\"\n",
    "\n",
    "from __future__ import annotations\n",
    "\n",
    "import argparse\n",
    "import json\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "from pose_ai.pose import PoseEstimator\n",
    "from pose_ai.service import estimate_poses_for_directory, estimate_poses_from_manifest\n",
    "\n",
    "\n",
    "def build_parser() -> argparse.ArgumentParser:\n",
    "    parser = argparse.ArgumentParser(description=\"Run MediaPipe pose estimation on frame manifests.\")\n",
    "    group = parser.add_mutually_exclusive_group(required=True)\n",
    "    group.add_argument(\"--manifest\", type=Path, help=\"Path to a manifest.json file.\")\n",
    "    group.add_argument(\"--frames-root\", type=Path, help=\"Directory containing extracted frame folders.\")\n",
    "    parser.add_argument(\n",
    "        \"--json\",\n",
    "        action=\"store_true\",\n",
    "        help=\"Print pose results as JSON instead of a textual summary.\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--no-save\",\n",
    "        action=\"store_true\",\n",
    "        help=\"Disable writing pose_results.json files alongside frames.\",\n",
    "    )\n",
    "    return parser\n",
    "\n",
    "\n",
    "def frames_to_dict(frames):\n",
    "    return [\n",
    "        {\n",
    "            \"image_path\": str(frame.image_path),\n",
    "            \"timestamp_seconds\": frame.timestamp_seconds,\n",
    "            \"detection_score\": frame.detection_score,\n",
    "            \"landmarks\": [\n",
    "                {\n",
    "                    \"name\": landmark.name,\n",
    "                    \"x\": landmark.x,\n",
    "                    \"y\": landmark.y,\n",
    "                    \"z\": landmark.z,\n",
    "                    \"visibility\": landmark.visibility,\n",
    "                }\n",
    "                for landmark in frame.landmarks\n",
    "            ],\n",
    "        }\n",
    "        for frame in frames\n",
    "    ]\n",
    "\n",
    "\n",
    "def main() -> None:\n",
    "    parser = build_parser()\n",
    "    args = parser.parse_args()\n",
    "\n",
    "    estimator = PoseEstimator()\n",
    "    try:\n",
    "        if args.manifest:\n",
    "            frames = estimate_poses_from_manifest(\n",
    "                args.manifest,\n",
    "                estimator=estimator,\n",
    "                save_json=not args.no_save,\n",
    "            )\n",
    "            if args.json:\n",
    "                print(json.dumps(frames_to_dict(frames), indent=2))\n",
    "            else:\n",
    "                print(f\"Processed {len(frames)} frames from {args.manifest}\")\n",
    "        else:\n",
    "            results = estimate_poses_for_directory(\n",
    "                args.frames_root,\n",
    "                estimator=estimator,\n",
    "                save_json=not args.no_save,\n",
    "            )\n",
    "            if args.json:\n",
    "                payload = {manifest: frames_to_dict(frames) for manifest, frames in results.items()}\n",
    "                print(json.dumps(payload, indent=2))\n",
    "            else:\n",
    "                for manifest, frames in results.items():\n",
    "                    print(f\"{manifest}: {len(frames)} frames\")\n",
    "    except ModuleNotFoundError as exc:\n",
    "        parser.error(\n",
    "            f\"{exc}. Ensure mediapipe is installed in your environment (e.g. `pip install mediapipe`).\"\n",
    "        )\n",
    "    finally:\n",
    "        estimator.close()\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## scripts/run_segment_report.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"CLI to generate segment-level metrics.\"\"\"\n",
    "\n",
    "from __future__ import annotations\n",
    "\n",
    "import argparse\n",
    "\n",
    "from pose_ai.service import generate_segment_report\n",
    "\n",
    "\n",
    "def build_parser() -> argparse.ArgumentParser:\n",
    "    parser = argparse.ArgumentParser(description=\"Export segment metrics (COM, joints, contacts)\")\n",
    "    parser.add_argument(\"manifest\", type=str, help=\"Path to manifest.json\")\n",
    "    parser.add_argument(\"--holds\", type=str, help=\"Optional holds JSON path\", default=None)\n",
    "    return parser\n",
    "\n",
    "\n",
    "def main() -> None:\n",
    "    parser = build_parser()\n",
    "    args = parser.parse_args()\n",
    "    metrics = generate_segment_report(args.manifest, holds_path=Path(args.holds) if args.holds else None)\n",
    "    print(f\"Saved {len(metrics)} segments\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## scripts/run_segmentation.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"CLI to run rule-based segmentation over extracted frame manifests.\"\"\"\n",
    "\n",
    "from __future__ import annotations\n",
    "\n",
    "import argparse\n",
    "import json\n",
    "from pathlib import Path\n",
    "\n",
    "from pose_ai.service import segment_video_from_manifest, segment_videos_under_directory\n",
    "\n",
    "\n",
    "def build_parser() -> argparse.ArgumentParser:\n",
    "    parser = argparse.ArgumentParser(description=\"Produce rest/movement segments from frame manifests.\")\n",
    "    group = parser.add_mutually_exclusive_group(required=True)\n",
    "    group.add_argument(\"--manifest\", type=Path, help=\"Path to a manifest.json file.\")\n",
    "    group.add_argument(\"--frames-root\", type=Path, help=\"Directory containing subfolders with manifest.json files.\")\n",
    "    parser.add_argument(\"--json\", action=\"store_true\", help=\"Print segmentation results as JSON.\")\n",
    "    return parser\n",
    "\n",
    "\n",
    "def _segment_to_dict(segment):\n",
    "    return {\n",
    "        \"start_time\": segment.start_time,\n",
    "        \"end_time\": segment.end_time,\n",
    "        \"label\": segment.label,\n",
    "        \"duration\": segment.duration,\n",
    "        \"frame_indices\": segment.frame_indices,\n",
    "    }\n",
    "\n",
    "\n",
    "def main() -> None:\n",
    "    parser = build_parser()\n",
    "    args = parser.parse_args()\n",
    "\n",
    "    if args.manifest:\n",
    "        segments = segment_video_from_manifest(args.manifest)\n",
    "        if args.json:\n",
    "            print(json.dumps([_segment_to_dict(seg) for seg in segments], indent=2))\n",
    "        else:\n",
    "            print(f\"Segments for {args.manifest}:\")\n",
    "            for seg in segments:\n",
    "                print(\n",
    "                    f\"- {seg.label:9s} {seg.start_time:5.2f}s → {seg.end_time:5.2f}s \"\n",
    "                    f\"(duration {seg.duration:4.2f}s, frames {seg.frame_indices})\"\n",
    "                )\n",
    "    else:\n",
    "        results = segment_videos_under_directory(args.frames_root)\n",
    "        if args.json:\n",
    "            payload = {\n",
    "                manifest: [_segment_to_dict(seg) for seg in segments]\n",
    "                for manifest, segments in results.items()\n",
    "            }\n",
    "            print(json.dumps(payload, indent=2))\n",
    "        else:\n",
    "            for manifest, segments in results.items():\n",
    "                print(f\"Segments for {manifest}:\")\n",
    "                for seg in segments:\n",
    "                    print(\n",
    "                        f\"  - {seg.label:9s} {seg.start_time:5.2f}s → {seg.end_time:5.2f}s \"\n",
    "                        f\"(duration {seg.duration:4.2f}s, frames {seg.frame_indices})\"\n",
    "                    )\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## scripts/train_xgboost.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Train an XGBoost model on pose feature data (CLI).\"\"\"\n",
    "\n",
    "from __future__ import annotations\n",
    "\n",
    "import argparse\n",
    "from pathlib import Path\n",
    "\n",
    "from pose_ai.ml.xgb_trainer import TrainParams, params_from_dict, train_from_file\n",
    "\n",
    "\n",
    "def _parse_args() -> argparse.Namespace:\n",
    "    parser = argparse.ArgumentParser(description=\"Train XGBoost on pose feature rows.\")\n",
    "    parser.add_argument(\"features\", type=Path, help=\"Path to pose_features.json\")\n",
    "\n",
    "    # Data/label\n",
    "    parser.add_argument(\"--label-column\", default=\"detection_score\")\n",
    "    parser.add_argument(\"--label-threshold\", type=float, default=None)\n",
    "    parser.add_argument(\"--drop-columns\", nargs=\"*\", default=[\"image_path\"])\n",
    "    parser.add_argument(\"--task\", choices=[\"classification\", \"regression\"], default=\"classification\")\n",
    "    parser.add_argument(\"--test-size\", type=float, default=0.2)\n",
    "    parser.add_argument(\"--random-state\", type=int, default=42)\n",
    "\n",
    "    # XGBoost hyperparameters\n",
    "    parser.add_argument(\"--n-estimators\", type=int, default=300)\n",
    "    parser.add_argument(\"--learning-rate\", type=float, default=0.05)\n",
    "    parser.add_argument(\"--max-depth\", type=int, default=4)\n",
    "    parser.add_argument(\"--subsample\", type=float, default=0.8)\n",
    "    parser.add_argument(\"--colsample-bytree\", type=float, default=0.8)\n",
    "    parser.add_argument(\"--scale-pos-weight\", type=float, default=None)\n",
    "    parser.add_argument(\"--n-jobs\", type=int, default=0)\n",
    "    parser.add_argument(\"--tree-method\", default=None, help=\"e.g., hist or gpu_hist\")\n",
    "\n",
    "    # Training behaviour\n",
    "    parser.add_argument(\"--early-stopping-rounds\", type=int, default=30)\n",
    "    parser.add_argument(\"--eval-metric-cls\", default=\"logloss\")\n",
    "    parser.add_argument(\"--eval-metric-reg\", default=\"rmse\")\n",
    "\n",
    "    # Outputs\n",
    "    parser.add_argument(\"--model-out\", type=Path, default=Path(\"models/xgb_pose.json\"))\n",
    "    parser.add_argument(\"--metrics-out\", type=Path, default=None)\n",
    "    parser.add_argument(\"--feature-out\", type=Path, default=None)\n",
    "    parser.add_argument(\"--importance-out\", type=Path, default=None)\n",
    "    return parser.parse_args()\n",
    "\n",
    "\n",
    "def main() -> None:\n",
    "    args = _parse_args()\n",
    "    params = params_from_dict(\n",
    "        {\n",
    "            \"task\": args.task,\n",
    "            \"label_column\": args.label_column,\n",
    "            \"label_threshold\": args.label_threshold,\n",
    "            \"drop_columns\": args.drop_columns,\n",
    "            \"test_size\": args.test_size,\n",
    "            \"random_state\": args.random_state,\n",
    "            \"n_estimators\": args.n_estimators,\n",
    "            \"learning_rate\": args.learning_rate,\n",
    "            \"max_depth\": args.max_depth,\n",
    "            \"subsample\": args.subsample,\n",
    "            \"colsample_bytree\": args.colsample_bytree,\n",
    "            \"scale_pos_weight\": args.scale_pos_weight,\n",
    "            \"n_jobs\": args.n_jobs,\n",
    "            \"tree_method\": args.tree_method,\n",
    "            \"early_stopping_rounds\": args.early_stopping_rounds,\n",
    "            \"eval_metric_cls\": args.eval_metric_cls,\n",
    "            \"eval_metric_reg\": args.eval_metric_reg,\n",
    "            \"model_out\": args.model_out,\n",
    "            \"metrics_out\": args.metrics_out,\n",
    "            \"feature_out\": args.feature_out,\n",
    "            \"importance_out\": args.importance_out,\n",
    "        }\n",
    "    )\n",
    "\n",
    "    metrics = train_from_file(args.features, params)\n",
    "    print(f\"Model saved to {params.model_out}\")\n",
    "    print(\"Metrics:\", metrics)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## scripts/visualize_pose.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Generate pose visualization overlays from pose_results.json.\"\"\"\n",
    "\n",
    "from __future__ import annotations\n",
    "\n",
    "import argparse\n",
    "import json\n",
    "from pathlib import Path\n",
    "\n",
    "import cv2\n",
    "\n",
    "try:  # optional dependency for connection definitions\n",
    "    from mediapipe.python.solutions.pose import PoseLandmark, POSE_CONNECTIONS\n",
    "except ModuleNotFoundError:  # fallback if mediapipe not installed\n",
    "    PoseLandmark = None\n",
    "    POSE_CONNECTIONS = []\n",
    "\n",
    "\n",
    "DEFAULT_CONNECTIONS = [\n",
    "    (\"left_shoulder\", \"right_shoulder\"),\n",
    "    (\"left_shoulder\", \"left_elbow\"),\n",
    "    (\"left_elbow\", \"left_wrist\"),\n",
    "    (\"right_shoulder\", \"right_elbow\"),\n",
    "    (\"right_elbow\", \"right_wrist\"),\n",
    "    (\"left_hip\", \"right_hip\"),\n",
    "    (\"left_shoulder\", \"left_hip\"),\n",
    "    (\"right_shoulder\", \"right_hip\"),\n",
    "    (\"left_hip\", \"left_knee\"),\n",
    "    (\"left_knee\", \"left_ankle\"),\n",
    "    (\"right_hip\", \"right_knee\"),\n",
    "    (\"right_knee\", \"right_ankle\"),\n",
    "]\n",
    "\n",
    "\n",
    "def build_parser() -> argparse.ArgumentParser:\n",
    "    parser = argparse.ArgumentParser(description=\"Visualize pose landmarks on extracted frames.\")\n",
    "    parser.add_argument(\"pose_results\", type=Path, help=\"Path to pose_results.json\")\n",
    "    parser.add_argument(\n",
    "        \"--output\", type=Path, default=None,\n",
    "        help=\"Directory to write visualized images (defaults to frame directory).\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--include-missing\", action=\"store_true\",\n",
    "        help=\"Include frames even when detection score/visibility is low.\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--min-score\",\n",
    "        type=float,\n",
    "        default=0.3,\n",
    "        help=\"Minimum frame detection score required for visualization (default: 0.3).\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--min-visibility\",\n",
    "        type=float,\n",
    "        default=0.2,\n",
    "        help=\"Minimum landmark visibility to draw a point/connection (default: 0.2).\",\n",
    "    )\n",
    "    return parser\n",
    "\n",
    "\n",
    "def _get_connections():\n",
    "    if PoseLandmark is None or not POSE_CONNECTIONS:\n",
    "        return DEFAULT_CONNECTIONS\n",
    "    connections = []\n",
    "    for a_idx, b_idx in POSE_CONNECTIONS:\n",
    "        connections.append((PoseLandmark(a_idx).name.lower(), PoseLandmark(b_idx).name.lower()))\n",
    "    return connections\n",
    "\n",
    "\n",
    "def visualize_pose_results(\n",
    "    pose_results_path: Path,\n",
    "    output_dir: Path | None = None,\n",
    "    *,\n",
    "    include_missing: bool = False,\n",
    "    min_score: float = 0.3,\n",
    "    min_visibility: float = 0.2,\n",
    ") -> int:\n",
    "    payload = json.loads(pose_results_path.read_text(encoding=\"utf-8\"))\n",
    "    frames = payload.get(\"frames\", [])\n",
    "    count = 0\n",
    "    connections = _get_connections()\n",
    "\n",
    "    for frame in frames:\n",
    "        image_path = Path(frame[\"image_path\"])\n",
    "        image = cv2.imread(str(image_path))\n",
    "        if image is None:\n",
    "            continue\n",
    "        height, width = image.shape[:2]\n",
    "        landmarks = frame.get(\"landmarks\", [])\n",
    "        detection_score = float(frame.get(\"detection_score\", 0.0))\n",
    "        if not include_missing and detection_score < min_score:\n",
    "            continue\n",
    "        if not landmarks and not include_missing:\n",
    "            continue\n",
    "\n",
    "        points = {}\n",
    "        for landmark in landmarks:\n",
    "            x = int(landmark[\"x\"] * width)\n",
    "            y = int(landmark[\"y\"] * height)\n",
    "            if landmark.get(\"visibility\", 1.0) < min_visibility:\n",
    "                continue\n",
    "            points[landmark[\"name\"]] = (x, y)\n",
    "            cv2.circle(image, (x, y), 4, (0, 255, 0), -1)\n",
    "\n",
    "        for start, end in connections:\n",
    "            if start in points and end in points:\n",
    "                cv2.line(image, points[start], points[end], (255, 0, 0), 2)\n",
    "\n",
    "        target_dir = output_dir or image_path.parent / \"visualized\"\n",
    "        target_dir.mkdir(parents=True, exist_ok=True)\n",
    "        out_path = target_dir / f\"{image_path.stem}_viz{image_path.suffix}\"\n",
    "        cv2.imwrite(str(out_path), image)\n",
    "        count += 1\n",
    "    return count\n",
    "\n",
    "\n",
    "def main() -> None:\n",
    "    parser = build_parser()\n",
    "    args = parser.parse_args()\n",
    "    processed = visualize_pose_results(\n",
    "        args.pose_results,\n",
    "        args.output,\n",
    "        include_missing=args.include_missing,\n",
    "        min_score=args.min_score,\n",
    "        min_visibility=args.min_visibility,\n",
    "    )\n",
    "    print(f\"Saved {processed} annotated frames\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82eab9b7",
   "metadata": {},
   "source": [
    "## Wall Angle Estimation Module\n",
    "\n",
    "Automatic wall angle estimation using Hough line detection and PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45dd4caf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# WALL ANGLE ESTIMATION\n",
    "# ============================================================================\n",
    "# Purpose: Automatically estimate climbing wall angle from video frames\n",
    "# Method: Combines Hough line detection with optional PCA on hold positions\n",
    "# Output: Angle in degrees (0 = horizontal, 90 = vertical wall)\n",
    "# ============================================================================\n",
    "\n",
    "from __future__ import annotations\n",
    "from dataclasses import dataclass\n",
    "from pathlib import Path\n",
    "from typing import List, Optional, Sequence, Tuple\n",
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "# Data structure for wall angle estimation results\n",
    "@dataclass(slots=True)\n",
    "class WallAngleResult:\n",
    "    angle_degrees: float | None\n",
    "    confidence: float\n",
    "    method: str\n",
    "    hough_lines: List[Tuple[Tuple[int, int], Tuple[int, int]]]\n",
    "    pca_angle: float | None = None\n",
    "\n",
    "    def as_dict(self) -> dict[str, object]:\n",
    "        return {\n",
    "            \"angle_degrees\": self.angle_degrees,\n",
    "            \"confidence\": self.confidence,\n",
    "            \"method\": self.method,\n",
    "            \"pca_angle\": self.pca_angle,\n",
    "            \"hough_line_count\": len(self.hough_lines),\n",
    "        }\n",
    "\n",
    "\n",
    "def estimate_wall_angle(\n",
    "    image_path: Path | str,\n",
    "    *,\n",
    "    hold_centers: Optional[list] = None,\n",
    "    canny_threshold1: int = 50,\n",
    "    canny_threshold2: int = 150,\n",
    "    hough_threshold: int = 120,\n",
    ") -> WallAngleResult:\n",
    "    \"\"\"\n",
    "    Estimate wall angle from a single frame image.\n",
    "    \n",
    "    Algorithm:\n",
    "    1. Convert image to grayscale\n",
    "    2. Apply Canny edge detection\n",
    "    3. Use Hough line transform to find dominant lines\n",
    "    4. Calculate mean angle from detected lines\n",
    "    5. Optionally refine with PCA on hold center coordinates\n",
    "    6. Blend estimates if both methods agree\n",
    "    \n",
    "    Parameters:\n",
    "        image_path: Path to frame image\n",
    "        hold_centers: Optional list of (x, y) hold coordinates for PCA refinement\n",
    "        canny_threshold1: Lower threshold for Canny edge detection\n",
    "        canny_threshold2: Upper threshold for Canny edge detection\n",
    "        hough_threshold: Minimum votes for Hough line detection\n",
    "        \n",
    "    Returns:\n",
    "        WallAngleResult with angle estimate and confidence score\n",
    "    \"\"\"\n",
    "    path = Path(image_path)\n",
    "    image = cv2.imread(str(path))\n",
    "    if image is None:\n",
    "        return WallAngleResult(\n",
    "            angle_degrees=None, \n",
    "            confidence=0.0, \n",
    "            method=\"load-error\", \n",
    "            hough_lines=[]\n",
    "        )\n",
    "    \n",
    "    # Step 1: Prepare image for edge detection\n",
    "    gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "    edges = cv2.Canny(gray, canny_threshold1, canny_threshold2, L2gradient=True)\n",
    "    \n",
    "    # Step 2: Detect lines using probabilistic Hough transform\n",
    "    lines_p = cv2.HoughLinesP(\n",
    "        edges, \n",
    "        rho=1, \n",
    "        theta=np.pi/180.0, \n",
    "        threshold=hough_threshold,\n",
    "        minLineLength=60, \n",
    "        maxLineGap=10\n",
    "    )\n",
    "    \n",
    "    # Step 3: Extract line endpoints\n",
    "    normalized_lines = []\n",
    "    if lines_p is not None:\n",
    "        for line in lines_p:\n",
    "            x1, y1, x2, y2 = line[0]\n",
    "            normalized_lines.append(((int(x1), int(y1)), (int(x2), int(y2))))\n",
    "    \n",
    "    # Step 4: Compute angle from Hough lines\n",
    "    angles = []\n",
    "    for (x1, y1), (x2, y2) in normalized_lines:\n",
    "        dx, dy = x2 - x1, y2 - y1\n",
    "        if abs(dx) < 1e-6 and abs(dy) < 1e-6:\n",
    "            continue\n",
    "        angle = float(np.degrees(np.arctan2(dy, dx)))\n",
    "        if angle < 0:\n",
    "            angle += 180.0\n",
    "        angles.append(angle)\n",
    "    \n",
    "    hough_angle = None\n",
    "    if angles:\n",
    "        # Use circular mean for angle averaging\n",
    "        radians = np.radians(angles)\n",
    "        mean = float(np.degrees(np.arctan2(np.sum(np.sin(radians)), np.sum(np.cos(radians)))))\n",
    "        if mean < 0:\n",
    "            mean += 180.0\n",
    "        hough_angle = mean\n",
    "    \n",
    "    # Step 5: Optional PCA refinement using hold positions\n",
    "    pca_angle_value = None\n",
    "    if hold_centers:\n",
    "        pts = np.array(list(hold_centers), dtype=float)\n",
    "        # Scale normalized coordinates to pixel space\n",
    "        if pts.max() <= 1.2:\n",
    "            h, w = gray.shape[:2]\n",
    "            pts[:, 0] *= w\n",
    "            pts[:, 1] *= h\n",
    "        if pts.shape[0] >= 3:\n",
    "            centered = pts - np.mean(pts, axis=0, keepdims=True)\n",
    "            cov = np.cov(centered.T)\n",
    "            eigvals, eigvecs = np.linalg.eig(cov)\n",
    "            idx = int(np.argmax(eigvals))\n",
    "            principal = eigvecs[:, idx]\n",
    "            pca_angle_value = float(np.degrees(np.arctan2(principal[1], principal[0])))\n",
    "            if pca_angle_value < 0:\n",
    "                pca_angle_value += 180.0\n",
    "    \n",
    "    # Step 6: Combine estimates with confidence weighting\n",
    "    if hough_angle is not None and pca_angle_value is not None:\n",
    "        diff = abs(hough_angle - pca_angle_value)\n",
    "        if diff < 15.0:  # Agreement threshold\n",
    "            final_angle = (hough_angle + pca_angle_value) / 2.0\n",
    "            method, confidence = \"hough+pca\", 0.9\n",
    "        else:  # Disagreement - prefer Hough but lower confidence\n",
    "            final_angle, method, confidence = hough_angle, \"hough\", 0.6\n",
    "    elif hough_angle is not None:\n",
    "        final_angle = hough_angle\n",
    "        confidence = 0.7 if len(normalized_lines) >= 5 else 0.5\n",
    "        method = \"hough\"\n",
    "    else:\n",
    "        final_angle = pca_angle_value\n",
    "        confidence = 0.4 if pca_angle_value is not None else 0.0\n",
    "        method = \"pca\" if pca_angle_value is not None else \"none\"\n",
    "    \n",
    "    return WallAngleResult(\n",
    "        angle_degrees=final_angle,\n",
    "        confidence=confidence,\n",
    "        method=method,\n",
    "        hough_lines=normalized_lines,\n",
    "        pca_angle=pca_angle_value,\n",
    "    )\n",
    "\n",
    "\n",
    "# Module loaded confirmation\n",
    "print(\"Wall angle estimation module loaded\")\n",
    "print(\"Usage: estimate_wall_angle(image_path, hold_centers=[(x1,y1), ...])\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40238da1",
   "metadata": {},
   "source": [
    "## Hold Detection and Clustering Module\n",
    "\n",
    "YOLO-based hold detection with DBSCAN clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23a8769b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# HOLD DETECTION AND CLUSTERING\n",
    "# ============================================================================\n",
    "# Purpose: Detect climbing holds from frames and cluster into stable positions\n",
    "# Method: YOLO object detection + DBSCAN spatial clustering\n",
    "# Output: List of unique holds with normalized coordinates\n",
    "# ============================================================================\n",
    "\n",
    "from __future__ import annotations\n",
    "from dataclasses import dataclass\n",
    "from pathlib import Path\n",
    "from typing import List, Sequence\n",
    "import json\n",
    "import numpy as np\n",
    "\n",
    "try:\n",
    "    from ultralytics import YOLO as UltralyticsYOLO\n",
    "    YOLO_AVAILABLE = True\n",
    "except ModuleNotFoundError:\n",
    "    UltralyticsYOLO = None\n",
    "    YOLO_AVAILABLE = False\n",
    "    print(\"Warning: ultralytics not installed\")\n",
    "    print(\"Install with: pip install ultralytics\")\n",
    "\n",
    "\n",
    "# Data structures for hold detection\n",
    "@dataclass(slots=True)\n",
    "class HoldDetection:\n",
    "    \"\"\"Single hold detection from one frame\"\"\"\n",
    "    frame_index: int\n",
    "    label: str\n",
    "    confidence: float\n",
    "    x_center: float  # normalized 0-1\n",
    "    y_center: float\n",
    "    width: float\n",
    "    height: float\n",
    "\n",
    "\n",
    "@dataclass(slots=True)\n",
    "class ClusteredHold:\n",
    "    \"\"\"Aggregated hold position from multiple detections\"\"\"\n",
    "    hold_id: str\n",
    "    label: str\n",
    "    x: float\n",
    "    y: float\n",
    "    radius: float\n",
    "    detections: int\n",
    "    avg_confidence: float\n",
    "\n",
    "    def as_dict(self) -> dict[str, object]:\n",
    "        return {\n",
    "            \"hold_id\": self.hold_id,\n",
    "            \"label\": self.label,\n",
    "            \"coords\": [self.x, self.y],\n",
    "            \"radius\": self.radius,\n",
    "            \"detections\": self.detections,\n",
    "            \"avg_confidence\": self.avg_confidence,\n",
    "            \"normalized\": True,\n",
    "        }\n",
    "\n",
    "\n",
    "def detect_holds(\n",
    "    image_paths: Sequence[Path],\n",
    "    *,\n",
    "    model_name: str = \"yolov8n.pt\",\n",
    "    device: str | None = None,\n",
    "    hold_labels: tuple = (\"hold\", \"foot_hold\", \"volume\", \"jug\", \"crimp\", \"sloper\", \"pinch\"),\n",
    ") -> List[HoldDetection]:\n",
    "    \"\"\"\n",
    "    Run YOLO object detection to find holds in frame images.\n",
    "    \n",
    "    Process:\n",
    "    1. Load YOLO model (pre-trained or fine-tuned)\n",
    "    2. Run inference on all frames in batch\n",
    "    3. Filter detections to hold-related classes only\n",
    "    4. Normalize bounding box coordinates to [0,1] range\n",
    "    \n",
    "    Parameters:\n",
    "        image_paths: List of frame image paths\n",
    "        model_name: YOLO model weights file (e.g., 'yolov8n.pt' or custom)\n",
    "        device: Torch device ('cpu', 'cuda:0', etc.)\n",
    "        hold_labels: Tuple of class labels to keep\n",
    "        \n",
    "    Returns:\n",
    "        List of HoldDetection objects with normalized coordinates\n",
    "    \"\"\"\n",
    "    if not YOLO_AVAILABLE:\n",
    "        print(\"YOLO not available - skipping hold detection\")\n",
    "        return []\n",
    "    \n",
    "    model = UltralyticsYOLO(model_name)\n",
    "    if not image_paths:\n",
    "        return []\n",
    "    \n",
    "    # Run batch inference\n",
    "    results = model.predict(\n",
    "        source=[str(p) for p in image_paths], \n",
    "        device=device, \n",
    "        imgsz=640, \n",
    "        stream=False, \n",
    "        verbose=False\n",
    "    )\n",
    "    \n",
    "    detections = []\n",
    "    \n",
    "    # Process each frame result\n",
    "    for frame_idx, result in enumerate(results):\n",
    "        boxes = getattr(result, \"boxes\", None)\n",
    "        if boxes is None:\n",
    "            continue\n",
    "        \n",
    "        # Extract detection data (move from GPU if needed)\n",
    "        xyxy = boxes.xyxy.cpu().tolist() if hasattr(boxes.xyxy, \"cpu\") else boxes.xyxy.tolist()\n",
    "        cls = boxes.cls.cpu().tolist() if hasattr(boxes.cls, \"cpu\") else boxes.cls.tolist()\n",
    "        conf = boxes.conf.cpu().tolist() if hasattr(boxes.conf, \"cpu\") else boxes.conf.tolist()\n",
    "        names = result.names or {}\n",
    "        \n",
    "        # Filter and normalize hold detections\n",
    "        for box_idx, bbox in enumerate(xyxy):\n",
    "            class_idx = int(cls[box_idx]) if box_idx < len(cls) else -1\n",
    "            label = str(names.get(class_idx, class_idx)).lower()\n",
    "            confidence = float(conf[box_idx]) if box_idx < len(conf) else 0.0\n",
    "            \n",
    "            # Keep only hold-related classes\n",
    "            if label not in hold_labels:\n",
    "                continue\n",
    "            \n",
    "            # Convert bbox to normalized center + size\n",
    "            x1, y1, x2, y2 = (float(v) for v in bbox[:4])\n",
    "            width = max(1e-6, x2 - x1)\n",
    "            height = max(1e-6, y2 - y1)\n",
    "            h, w = result.orig_shape\n",
    "            \n",
    "            detections.append(HoldDetection(\n",
    "                frame_index=frame_idx,\n",
    "                label=label,\n",
    "                confidence=confidence,\n",
    "                x_center=(x1 + width/2.0) / w,\n",
    "                y_center=(y1 + height/2.0) / h,\n",
    "                width=width / w,\n",
    "                height=height / h,\n",
    "            ))\n",
    "    \n",
    "    return detections\n",
    "\n",
    "\n",
    "def cluster_holds(\n",
    "    detections: Sequence[HoldDetection], \n",
    "    *, \n",
    "    eps: float = 0.03, \n",
    "    min_samples: int = 3\n",
    ") -> List[ClusteredHold]:\n",
    "    \"\"\"\n",
    "    Cluster hold detections across frames to find stable hold positions.\n",
    "    \n",
    "    Algorithm:\n",
    "    1. Extract 2D coordinates from all detections\n",
    "    2. Apply DBSCAN density-based clustering\n",
    "    3. For each cluster, compute centroid and statistics\n",
    "    4. Assign unique hold IDs to each cluster\n",
    "    \n",
    "    Parameters:\n",
    "        detections: List of hold detections from all frames\n",
    "        eps: DBSCAN epsilon (max distance between points in cluster)\n",
    "        min_samples: Minimum detections required to form cluster\n",
    "        \n",
    "    Returns:\n",
    "        List of ClusteredHold objects representing unique holds\n",
    "    \"\"\"\n",
    "    if not detections:\n",
    "        return []\n",
    "    \n",
    "    points = np.array([[d.x_center, d.y_center] for d in detections], dtype=float)\n",
    "    labels_arr = np.array([d.label for d in detections], dtype=object)\n",
    "    confidences = np.array([d.confidence for d in detections], dtype=float)\n",
    "    \n",
    "    # Apply DBSCAN clustering\n",
    "    try:\n",
    "        from sklearn.cluster import DBSCAN\n",
    "        clustering = DBSCAN(eps=eps, min_samples=min_samples).fit(points)\n",
    "        cluster_ids = clustering.labels_\n",
    "    except Exception:\n",
    "        # Fallback: no clustering\n",
    "        cluster_ids = np.full(points.shape[0], -1, dtype=int)\n",
    "    \n",
    "    unique_ids = [cid for cid in sorted(set(int(c) for c in cluster_ids)) if cid >= 0]\n",
    "    clustered = []\n",
    "    \n",
    "    if not unique_ids:\n",
    "        # Fallback: treat each detection as unique hold\n",
    "        for idx, d in enumerate(detections):\n",
    "            clustered.append(ClusteredHold(\n",
    "                hold_id=f\"hold_{idx}\",\n",
    "                label=d.label,\n",
    "                x=d.x_center,\n",
    "                y=d.y_center,\n",
    "                radius=max(d.width, d.height) / 2.0,\n",
    "                detections=1,\n",
    "                avg_confidence=d.confidence,\n",
    "            ))\n",
    "        return clustered\n",
    "    \n",
    "    # Aggregate clusters\n",
    "    for cid in unique_ids:\n",
    "        mask = cluster_ids == cid\n",
    "        cluster_pts = points[mask]\n",
    "        cluster_labels = labels_arr[mask]\n",
    "        cluster_conf = confidences[mask]\n",
    "        \n",
    "        # Compute cluster centroid\n",
    "        x_mean = float(cluster_pts[:, 0].mean())\n",
    "        y_mean = float(cluster_pts[:, 1].mean())\n",
    "        \n",
    "        # Compute cluster radius (mean distance to centroid)\n",
    "        dists = np.linalg.norm(cluster_pts - np.array([[x_mean, y_mean]]), axis=1)\n",
    "        radius = float(dists.mean() + 0.01)\n",
    "        \n",
    "        # Find dominant label\n",
    "        lbl_values, counts = np.unique(cluster_labels, return_counts=True)\n",
    "        dominant_label = str(lbl_values[int(np.argmax(counts))])\n",
    "        \n",
    "        clustered.append(ClusteredHold(\n",
    "            hold_id=f\"hold_{cid}\",\n",
    "            label=dominant_label,\n",
    "            x=x_mean,\n",
    "            y=y_mean,\n",
    "            radius=radius,\n",
    "            detections=int(mask.sum()),\n",
    "            avg_confidence=float(cluster_conf.mean()),\n",
    "        ))\n",
    "    \n",
    "    return clustered\n",
    "\n",
    "\n",
    "def export_holds_json(\n",
    "    clustered: Sequence[ClusteredHold], \n",
    "    *, \n",
    "    output_path: Path | str\n",
    ") -> Path:\n",
    "    \"\"\"\n",
    "    Export clustered holds to JSON file.\n",
    "    \n",
    "    Output format:\n",
    "    {\n",
    "        \"hold_0\": {\"coords\": [x, y], \"label\": \"jug\", ...},\n",
    "        \"hold_1\": {\"coords\": [x, y], \"label\": \"crimp\", ...},\n",
    "        ...\n",
    "    }\n",
    "    \"\"\"\n",
    "    path = Path(output_path)\n",
    "    path.parent.mkdir(parents=True, exist_ok=True)\n",
    "    payload = {hold.hold_id: hold.as_dict() for hold in clustered}\n",
    "    path.write_text(json.dumps(payload, indent=2), encoding=\"utf-8\")\n",
    "    return path\n",
    "\n",
    "\n",
    "# Module loaded confirmation\n",
    "print(\"Hold detection and clustering module loaded\")\n",
    "print(f\"YOLO available: {YOLO_AVAILABLE}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5394151",
   "metadata": {},
   "source": [
    "## Efficiency Scoring and Recommendation Module\n",
    "\n",
    "Heuristic-based efficiency scoring and next hold recommendation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2ad8595",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# EFFICIENCY SCORING AND RECOMMENDATION\n",
    "# ============================================================================\n",
    "# Purpose: Calculate movement efficiency and suggest next holds\n",
    "# Method: Multi-component heuristic scoring + proximity-based ranking\n",
    "# Output: Efficiency score (0-1) and ranked hold suggestions\n",
    "# ============================================================================\n",
    "\n",
    "from __future__ import annotations\n",
    "from dataclasses import dataclass\n",
    "from typing import List, Sequence\n",
    "import math\n",
    "\n",
    "# Weight configuration for efficiency components\n",
    "WEIGHTS = {\n",
    "    \"detection\": 0.30,  # Pose detection quality\n",
    "    \"joint\": 0.20,      # Joint angle efficiency (less extreme = better)\n",
    "    \"com\": 0.20,        # Center of mass stability\n",
    "    \"contact\": 0.20,    # Contact point stability\n",
    "    \"hip\": 0.10,        # Hip alignment with wall\n",
    "}\n",
    "\n",
    "\n",
    "@dataclass(slots=True)\n",
    "class EfficiencyResult:\n",
    "    \"\"\"Efficiency score with component breakdown\"\"\"\n",
    "    score: float\n",
    "    components: dict[str, float]\n",
    "\n",
    "    def as_dict(self) -> dict[str, float]:\n",
    "        payload = {\"score\": self.score}\n",
    "        payload.update(self.components)\n",
    "        return payload\n",
    "\n",
    "\n",
    "def _safe_float(value) -> float:\n",
    "    \"\"\"Convert value to float, return NaN on failure\"\"\"\n",
    "    try:\n",
    "        return float(value)\n",
    "    except Exception:\n",
    "        return float(\"nan\")\n",
    "\n",
    "\n",
    "def efficiency_from_frames(\n",
    "    frame_rows: Sequence[dict[str, object]], \n",
    "    window: int = 5\n",
    ") -> EfficiencyResult:\n",
    "    \"\"\"\n",
    "    Calculate efficiency score from pose feature frames.\n",
    "    \n",
    "    Algorithm:\n",
    "    1. Take last N frames (window) for recent movement context\n",
    "    2. Compute 5 components:\n",
    "       - detection: Average pose detection quality\n",
    "       - joint: Joint angle efficiency (normalized by 180 degrees)\n",
    "       - com: Center of mass stability (inverse variance)\n",
    "       - contact: Limb contact stability (hold consistency)\n",
    "       - hip: Hip alignment with wall angle\n",
    "    3. Weighted average of components\n",
    "    \n",
    "    Parameters:\n",
    "        frame_rows: List of feature dictionaries (from pose_features.json)\n",
    "        window: Number of recent frames to analyze\n",
    "        \n",
    "    Returns:\n",
    "        EfficiencyResult with overall score and component breakdown\n",
    "    \"\"\"\n",
    "    if not frame_rows:\n",
    "        return EfficiencyResult(score=float(\"nan\"), components={})\n",
    "    \n",
    "    # Use recent frames for temporal context\n",
    "    recent = list(frame_rows[-window:]) if len(frame_rows) >= window else list(frame_rows)\n",
    "    \n",
    "    # Component 1: Detection quality\n",
    "    detection_scores = [\n",
    "        _safe_float(row.get(\"detection_score\")) \n",
    "        for row in recent \n",
    "        if row.get(\"detection_score\") is not None\n",
    "    ]\n",
    "    detection_component = (\n",
    "        float(sum(detection_scores) / len(detection_scores)) \n",
    "        if detection_scores else 0.0\n",
    "    )\n",
    "    \n",
    "    # Component 2: Joint efficiency (lower angles = more efficient)\n",
    "    joint_keys = [k for k in recent[-1].keys() if k.endswith(\"_angle\")]\n",
    "    joint_values = []\n",
    "    for row in recent:\n",
    "        for key in joint_keys:\n",
    "            v = row.get(key)\n",
    "            if isinstance(v, (int, float)):\n",
    "                joint_values.append(abs(float(v)))\n",
    "    \n",
    "    joint_component = (\n",
    "        1.0 - (sum(joint_values) / (len(joint_values) * 180.0)) \n",
    "        if joint_values else 0.5\n",
    "    )\n",
    "    \n",
    "    # Component 3: COM stability (perpendicular to wall)\n",
    "    com_perp_vals = [\n",
    "        _safe_float(row.get(\"com_perp_wall\")) \n",
    "        for row in recent \n",
    "        if row.get(\"com_perp_wall\") is not None\n",
    "    ]\n",
    "    \n",
    "    if len(com_perp_vals) >= 2:\n",
    "        mean_val = sum(com_perp_vals) / len(com_perp_vals)\n",
    "        variance = sum((v - mean_val) ** 2 for v in com_perp_vals) / len(com_perp_vals)\n",
    "        max_var = max(variance, 1e-6)\n",
    "        com_component = 1.0 / (1.0 + variance / max_var)\n",
    "    else:\n",
    "        com_component = 0.5\n",
    "    \n",
    "    # Component 4: Contact stability (limbs staying on same holds)\n",
    "    contact_keys = [k for k in recent[-1].keys() if k.endswith(\"_target\")]\n",
    "    stable_counts = 0\n",
    "    total_contacts = 0\n",
    "    \n",
    "    if contact_keys and len(recent) >= 2:\n",
    "        prev_row = recent[0]\n",
    "        for row in recent[1:]:\n",
    "            for key in contact_keys:\n",
    "                if key in row and key in prev_row:\n",
    "                    total_contacts += 1\n",
    "                    if row.get(key) == prev_row.get(key):\n",
    "                        stable_counts += 1\n",
    "            prev_row = row\n",
    "    \n",
    "    contact_component = (stable_counts / total_contacts) if total_contacts else 0.5\n",
    "    \n",
    "    # Component 5: Hip alignment with wall\n",
    "    hip_alignment = _safe_float(recent[-1].get(\"hip_alignment_error\"))\n",
    "    hip_component = (\n",
    "        1.0 - (hip_alignment / 90.0) \n",
    "        if hip_alignment == hip_alignment  # Check for NaN\n",
    "        else 0.5\n",
    "    )\n",
    "    \n",
    "    # Weighted combination\n",
    "    score = (\n",
    "        WEIGHTS[\"detection\"] * detection_component\n",
    "        + WEIGHTS[\"joint\"] * joint_component\n",
    "        + WEIGHTS[\"com\"] * com_component\n",
    "        + WEIGHTS[\"contact\"] * contact_component\n",
    "        + WEIGHTS[\"hip\"] * hip_component\n",
    "    )\n",
    "    \n",
    "    return EfficiencyResult(\n",
    "        score=score,\n",
    "        components={\n",
    "            \"detection_component\": detection_component,\n",
    "            \"joint_component\": joint_component,\n",
    "            \"com_component\": com_component,\n",
    "            \"contact_component\": contact_component,\n",
    "            \"hip_component\": hip_component,\n",
    "        },\n",
    "    )\n",
    "\n",
    "\n",
    "def suggest_next_holds(\n",
    "    current_row: dict[str, object],\n",
    "    all_holds: Sequence[dict[str, object]],\n",
    "    *,\n",
    "    top_k: int = 3,\n",
    ") -> List[dict[str, object]]:\n",
    "    \"\"\"\n",
    "    Suggest next holds based on proximity and novelty.\n",
    "    \n",
    "    Ranking heuristic:\n",
    "        score = 0.7 * (1 - distance) + 0.3 * novelty\n",
    "        \n",
    "    Where:\n",
    "        - distance: Euclidean distance from COM to hold (normalized)\n",
    "        - novelty: 1 if hold not currently contacted, 0 otherwise\n",
    "    \n",
    "    Parameters:\n",
    "        current_row: Latest feature row (current climber state)\n",
    "        all_holds: List of available holds (from holds.json)\n",
    "        top_k: Number of holds to recommend\n",
    "        \n",
    "    Returns:\n",
    "        List of top-k hold dictionaries sorted by score\n",
    "    \"\"\"\n",
    "    com_x = _safe_float(current_row.get(\"com_x\"))\n",
    "    com_y = _safe_float(current_row.get(\"com_y\"))\n",
    "    \n",
    "    # Require valid COM position\n",
    "    if any(math.isnan(v) for v in (com_x, com_y)):\n",
    "        return []\n",
    "    \n",
    "    # Track currently used holds\n",
    "    used_targets = {\n",
    "        str(current_row.get(k))\n",
    "        for k in current_row.keys()\n",
    "        if k.endswith(\"_target\") and current_row.get(k)\n",
    "    }\n",
    "    \n",
    "    # Rank all holds\n",
    "    ranked = []\n",
    "    for hold in all_holds:\n",
    "        coords = hold.get(\"coords\")\n",
    "        if not isinstance(coords, (list, tuple)) or len(coords) != 2:\n",
    "            continue\n",
    "        \n",
    "        hx, hy = float(coords[0]), float(coords[1])\n",
    "        \n",
    "        # Calculate distance component\n",
    "        dist = math.sqrt((hx - com_x) ** 2 + (hy - com_y) ** 2)\n",
    "        dist_norm = min(dist, 1.0)\n",
    "        \n",
    "        # Calculate novelty component\n",
    "        novel = 0 if hold.get(\"hold_id\") in used_targets else 1\n",
    "        \n",
    "        # Combined score\n",
    "        score = 0.7 * (1.0 - dist_norm) + 0.3 * novel\n",
    "        ranked.append((score, hold))\n",
    "    \n",
    "    # Sort and return top-k\n",
    "    ranked.sort(key=lambda t: t[0], reverse=True)\n",
    "    return [hold for _, hold in ranked[:top_k]]\n",
    "\n",
    "\n",
    "# Module loaded confirmation\n",
    "print(\"Efficiency and recommendation module loaded\")\n",
    "print(\"Component weights:\", WEIGHTS)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ffacc58",
   "metadata": {},
   "source": [
    "## Complete Pipeline Demo\n",
    "\n",
    "End-to-end demonstration of all new features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59ce92aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# COMPLETE PIPELINE DEMONSTRATION\n",
    "# ============================================================================\n",
    "# Purpose: Show integration of all new features in end-to-end workflow\n",
    "# Steps:\n",
    "#   1. Hold detection and clustering\n",
    "#   2. Wall angle estimation\n",
    "#   3. Load pose features\n",
    "#   4. Calculate efficiency score\n",
    "#   5. Generate hold recommendations\n",
    "# ============================================================================\n",
    "\n",
    "from pathlib import Path\n",
    "import json\n",
    "\n",
    "# Configuration: Set your frame directory path\n",
    "FRAME_DIR = Path(\"data/frames/video01\")  # Change to your actual path\n",
    "manifest_path = FRAME_DIR / \"manifest.json\"\n",
    "pose_results_path = FRAME_DIR / \"pose_results.json\"\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"BetaMove Enhanced Pipeline Demo\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# ----------------------------------------------------------------------------\n",
    "# STEP 1: Hold Detection and Clustering\n",
    "# ----------------------------------------------------------------------------\n",
    "print(\"\\n[Step 1/5] Hold Detection and Clustering\")\n",
    "print(\"-\" * 60)\n",
    "\n",
    "if YOLO_AVAILABLE:\n",
    "    # Get sample frames (limit to 20 for demo speed)\n",
    "    image_paths = sorted(FRAME_DIR.glob(\"*.jpg\"))[:20]\n",
    "    \n",
    "    if image_paths:\n",
    "        # Run YOLO detection\n",
    "        holds_detections = detect_holds(image_paths, model_name=\"yolov8n.pt\")\n",
    "        print(f\"Detected {len(holds_detections)} hold instances across frames\")\n",
    "        \n",
    "        # Cluster detections into unique holds\n",
    "        clustered_holds = cluster_holds(holds_detections, eps=0.03, min_samples=2)\n",
    "        print(f\"Clustered into {len(clustered_holds)} unique holds\")\n",
    "        \n",
    "        if clustered_holds:\n",
    "            # Save to JSON\n",
    "            holds_json_path = export_holds_json(\n",
    "                clustered_holds, \n",
    "                output_path=FRAME_DIR / \"holds.json\"\n",
    "            )\n",
    "            print(f\"Exported to: {holds_json_path}\")\n",
    "            \n",
    "            # Display sample holds\n",
    "            print(\"\\nSample holds:\")\n",
    "            for h in clustered_holds[:3]:\n",
    "                print(f\"  {h.hold_id}: {h.label} at ({h.x:.3f}, {h.y:.3f})\")\n",
    "                print(f\"    confidence={h.avg_confidence:.2f}, detections={h.detections}\")\n",
    "    else:\n",
    "        print(\"Warning: No frame images found\")\n",
    "        clustered_holds = []\n",
    "else:\n",
    "    print(\"Warning: YOLO not available - skipping hold detection\")\n",
    "    clustered_holds = []\n",
    "\n",
    "# ----------------------------------------------------------------------------\n",
    "# STEP 2: Wall Angle Estimation\n",
    "# ----------------------------------------------------------------------------\n",
    "print(\"\\n[Step 2/5] Wall Angle Estimation\")\n",
    "print(\"-\" * 60)\n",
    "\n",
    "first_image = next(FRAME_DIR.glob(\"*.jpg\"), None)\n",
    "if first_image:\n",
    "    # Use hold centers for PCA refinement if available\n",
    "    hold_centers = [(h.x, h.y) for h in clustered_holds] if clustered_holds else None\n",
    "    \n",
    "    # Estimate wall angle\n",
    "    wall_result = estimate_wall_angle(first_image, hold_centers=hold_centers)\n",
    "    \n",
    "    if wall_result.angle_degrees is not None:\n",
    "        print(f\"Estimated angle: {wall_result.angle_degrees:.1f} degrees\")\n",
    "        print(f\"Confidence: {wall_result.confidence:.2f}\")\n",
    "        print(f\"Method: {wall_result.method}\")\n",
    "        print(f\"Hough lines detected: {len(wall_result.hough_lines)}\")\n",
    "        if wall_result.pca_angle:\n",
    "            print(f\"PCA angle: {wall_result.pca_angle:.1f} degrees\")\n",
    "    else:\n",
    "        print(\"Failed to estimate wall angle\")\n",
    "else:\n",
    "    print(\"Warning: No images found for wall angle estimation\")\n",
    "    wall_result = None\n",
    "\n",
    "# ----------------------------------------------------------------------------\n",
    "# STEP 3: Load Pose Features\n",
    "# ----------------------------------------------------------------------------\n",
    "print(\"\\n[Step 3/5] Loading Pose Features\")\n",
    "print(\"-\" * 60)\n",
    "\n",
    "features_path = FRAME_DIR / \"pose_features.json\"\n",
    "if features_path.exists():\n",
    "    with open(features_path) as f:\n",
    "        feature_rows = json.load(f)\n",
    "    \n",
    "    print(f\"Loaded {len(feature_rows)} feature rows\")\n",
    "    \n",
    "    # Check for new features\n",
    "    if feature_rows:\n",
    "        sample = feature_rows[0]\n",
    "        print(f\"Total features per row: {len(sample)}\")\n",
    "        \n",
    "        new_features = [\n",
    "            k for k in sample.keys() \n",
    "            if k in ['wall_angle', 'hip_alignment_error', 'com_along_wall', 'com_perp_wall']\n",
    "        ]\n",
    "        if new_features:\n",
    "            print(f\"New wall features present: {', '.join(new_features)}\")\n",
    "else:\n",
    "    print(\"Warning: pose_features.json not found\")\n",
    "    print(\"Tip: Run feature export with auto_wall_angle=True\")\n",
    "    feature_rows = []\n",
    "\n",
    "# ----------------------------------------------------------------------------\n",
    "# STEP 4: Calculate Efficiency Score\n",
    "# ----------------------------------------------------------------------------\n",
    "print(\"\\n[Step 4/5] Efficiency Score Calculation\")\n",
    "print(\"-\" * 60)\n",
    "\n",
    "if feature_rows:\n",
    "    # Calculate efficiency using recent frames\n",
    "    eff_result = efficiency_from_frames(feature_rows, window=5)\n",
    "    \n",
    "    print(f\"Overall Efficiency Score: {eff_result.score:.3f}\")\n",
    "    print(\"\\nComponent Breakdown:\")\n",
    "    for comp, val in eff_result.components.items():\n",
    "        print(f\"  {comp:20s}: {val:.3f}\")\n",
    "else:\n",
    "    print(\"Warning: No features available for efficiency calculation\")\n",
    "    eff_result = None\n",
    "\n",
    "# ----------------------------------------------------------------------------\n",
    "# STEP 5: Next Hold Recommendations\n",
    "# ----------------------------------------------------------------------------\n",
    "print(\"\\n[Step 5/5] Next Hold Recommendations\")\n",
    "print(\"-\" * 60)\n",
    "\n",
    "if feature_rows and clustered_holds:\n",
    "    # Convert holds to dictionary format\n",
    "    holds_dicts = [h.as_dict() for h in clustered_holds]\n",
    "    \n",
    "    # Get recommendations based on current state\n",
    "    next_holds = suggest_next_holds(feature_rows[-1], holds_dicts, top_k=3)\n",
    "    \n",
    "    print(f\"Top {len(next_holds)} recommended holds:\")\n",
    "    for i, hold in enumerate(next_holds, 1):\n",
    "        coords = hold.get('coords', [])\n",
    "        print(f\"  {i}. {hold.get('hold_id')} ({hold.get('label')})\")\n",
    "        print(f\"     Position: ({coords[0]:.3f}, {coords[1]:.3f})\")\n",
    "        print(f\"     Confidence: {hold.get('avg_confidence', 0):.2f}\")\n",
    "else:\n",
    "    print(\"Warning: Missing features or holds for recommendations\")\n",
    "\n",
    "# ----------------------------------------------------------------------------\n",
    "# Summary\n",
    "# ----------------------------------------------------------------------------\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"Demo Complete\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "print(\"\\nSummary:\")\n",
    "print(f\"  Holds detected: {len(clustered_holds)}\")\n",
    "if wall_result and wall_result.angle_degrees:\n",
    "    print(f\"  Wall angle: {wall_result.angle_degrees:.1f} degrees ({wall_result.method})\")\n",
    "else:\n",
    "    print(\"  Wall angle: Not available\")\n",
    "if eff_result:\n",
    "    print(f\"  Efficiency score: {eff_result.score:.3f}\")\n",
    "else:\n",
    "    print(\"  Efficiency score: Not available\")\n",
    "\n",
    "print(\"\\nTo run full pipeline with new features:\")\n",
    "print(\"  Option 1: Use web UI - upload video (auto-runs all features)\")\n",
    "print(\"  Option 2: CLI - python scripts/generate_holds_and_features.py <manifest_path>\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "6156 (py3.10)",
   "language": "python",
   "name": "6156-capstone"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
