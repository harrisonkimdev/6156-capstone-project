{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fcf437c3",
   "metadata": {},
   "source": [
    "# 6156 Capstone Project\n",
    "\n",
    "Group 5: Connor Lynch, Harrison Kim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1c4abe35",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fatal: destination path '6156-capstone-project' already exists and is not an empty directory.\n",
      "/Users/harrisonkim/code/repos/6156-capstone-project/notebooks/6156-capstone-project\n",
      "Cloned Repository!\n",
      "Current location: /Users/harrisonkim/code/repos/6156-capstone-project/notebooks/6156-capstone-project\n"
     ]
    }
   ],
   "source": [
    "# Clone directly from GitHub\n",
    "!git clone https://github.com/harrisonkimdev/6156-capstone-project.git\n",
    "%cd 6156-capstone-project\n",
    "\n",
    "import os\n",
    "print(\"Cloned Repository!\")\n",
    "print(f\"Current location: {os.getcwd()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f943e02",
   "metadata": {},
   "source": [
    "## Install required packages and configure pose_ai module path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e0969212",
   "metadata": {
    "tags": [
     "setup"
    ]
   },
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "unterminated string literal (detected at line 79) (498671747.py, line 79)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Cell \u001b[0;32mIn[3], line 79\u001b[0;36m\u001b[0m\n\u001b[0;31m    Current working directory: {Path.cwd()}\")\u001b[0m\n\u001b[0m                                           ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m unterminated string literal (detected at line 79)\n"
     ]
    }
   ],
   "source": [
    "# 1. Install packages (protobuf>=5.29.1 avoids Colab resolver conflicts)\n",
    "import os\n",
    "import sys\n",
    "import subprocess\n",
    "from pathlib import Path\n",
    "\n",
    "def find_repo_root() -> Path:\n",
    "    candidates = [\n",
    "        Path.cwd(),\n",
    "        Path.cwd() / '6156-capstone-project',\n",
    "        Path('/content/6156-capstone-project'),\n",
    "    ]\n",
    "    candidates.extend(Path.cwd().parents)\n",
    "    seen = set()\n",
    "    for candidate in candidates:\n",
    "        candidate = candidate.resolve()\n",
    "        if candidate in seen:\n",
    "            continue\n",
    "        seen.add(candidate)\n",
    "        if (candidate / 'src').exists():\n",
    "            return candidate\n",
    "    raise FileNotFoundError('Could not locate repo root containing src/. Make sure the repo is cloned.')\n",
    "\n",
    "repo_root = find_repo_root()\n",
    "if Path.cwd().resolve() != repo_root:\n",
    "    os.chdir(repo_root)\n",
    "    print(f'Changed working directory to repo root: {repo_root}')\n",
    "else:\n",
    "    print(f'Using existing working directory: {repo_root}')\n",
    "\n",
    "requirements_path = repo_root / 'requirements.txt'\n",
    "fallback_packages = [\n",
    "    'numpy',\n",
    "    'pandas',\n",
    "    'scikit-learn',\n",
    "    'xgboost',\n",
    "    'opencv-python',\n",
    "    'mediapipe',\n",
    "    'protobuf>=5.29.1,<7',\n",
    "]\n",
    "\n",
    "def _pip_install(*args):\n",
    "    cmd = [sys.executable, '-m', 'pip', *args]\n",
    "    print('Running:', ' '.join(cmd))\n",
    "    subprocess.run(cmd, check=True)\n",
    "\n",
    "_pip_install('install', '-q', '--upgrade', 'pip')\n",
    "if requirements_path.exists():\n",
    "    _pip_install('install', '-q', '-r', str(requirements_path))\n",
    "else:\n",
    "    print(f'⚠️ requirements.txt not found at {requirements_path}, installing fallback packages.')\n",
    "    _pip_install('install', '-q', *fallback_packages)\n",
    "\n",
    "# 2. Configure path to import pose_ai module\n",
    "# Add src folder to Python path\n",
    "src_path = repo_root / 'src'\n",
    "\n",
    "if src_path.exists():\n",
    "    if str(src_path) not in sys.path:\n",
    "        sys.path.insert(0, str(src_path))\n",
    "    os.environ.setdefault('PYTHONPATH', str(src_path) + (':' + os.environ.get('PYTHONPATH', '')))\n",
    "    print(f\"✓ src path added: {src_path}\")\n",
    "else:\n",
    "    print(f\"⚠️ Warning: src folder not found: {src_path}\")\n",
    "    print('   Please ensure the repository is cloned or src/ folder exists.')\n",
    "\n",
    "# 3. Verify pose_ai import\n",
    "try:\n",
    "    import pose_ai as _pose_ai\n",
    "    print(f\"✓ pose_ai module imported successfully! (version: {getattr(_pose_ai, '__version__', 'unknown')})\")\n",
    "except Exception as e:  # pylint: disable=broad-exception-caught\n",
    "    print(f\"✗ pose_ai import failed: {e}\")\n",
    "    print('  Solutions:')\n",
    "    print('  1. Verify the repository is cloned correctly')\n",
    "    print('  2. Check if src/pose_ai/ folder exists')\n",
    "    print('  3. Refer to \"How to Use in Google Colab\" cell above for repo setup')\n",
    "\n",
    "# Display current working directory\n",
    "print(f\"Current working directory: {Path.cwd()}\")\n",
    "print(f\"src added to Python path: {str(src_path) if src_path.exists() else 'N/A'}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scripts Python Collection\n",
    "\n",
    "This notebook aggregates the Python scripts from the `scripts` directory."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## scripts/extract_frames.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'pose_ai'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 9\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mlogging\u001b[39;00m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mpathlib\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Path\n\u001b[0;32m----> 9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mpose_ai\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdata\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m FrameExtractionResult, extract_frames_every_n_seconds, iter_video_files\n\u001b[1;32m     12\u001b[0m LOGGER \u001b[38;5;241m=\u001b[39m logging\u001b[38;5;241m.\u001b[39mgetLogger(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpose_ai.scripts.extract_frames\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     15\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mconfigure_logging\u001b[39m(verbose: \u001b[38;5;28mbool\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'pose_ai'"
     ]
    }
   ],
   "source": [
    "\"\"\"CLI for extracting frame sequences from climbing videos.\"\"\"\n",
    "\n",
    "from __future__ import annotations\n",
    "\n",
    "import argparse\n",
    "import logging\n",
    "from pathlib import Path\n",
    "\n",
    "from pose_ai.data import FrameExtractionResult, extract_frames_every_n_seconds, iter_video_files\n",
    "\n",
    "\n",
    "LOGGER = logging.getLogger(\"pose_ai.scripts.extract_frames\")\n",
    "\n",
    "\n",
    "def configure_logging(verbose: bool) -> None:\n",
    "    level = logging.DEBUG if verbose else logging.INFO\n",
    "    logging.basicConfig(\n",
    "        level=level,\n",
    "        format=\"%(asctime)s [%(levelname)s] %(name)s: %(message)s\",\n",
    "    )\n",
    "\n",
    "\n",
    "def extract_from_directory(\n",
    "    source_dir: Path,\n",
    "    *,\n",
    "    output_root: Path,\n",
    "    interval_seconds: float,\n",
    "    recursive: bool,\n",
    "    overwrite: bool,\n",
    "    write_manifest: bool,\n",
    ") -> list[FrameExtractionResult]:\n",
    "    results: list[FrameExtractionResult] = []\n",
    "    for video_path in iter_video_files(source_dir, recursive=recursive):\n",
    "        LOGGER.info(\"Processing %s\", video_path)\n",
    "        result = extract_frames_every_n_seconds(\n",
    "            video_path,\n",
    "            interval_seconds=interval_seconds,\n",
    "            output_root=output_root,\n",
    "            write_manifest=write_manifest,\n",
    "            overwrite=overwrite,\n",
    "        )\n",
    "        LOGGER.info(\n",
    "            \"Saved %d frames for %s\",\n",
    "            result.saved_frames,\n",
    "            video_path.name,\n",
    "        )\n",
    "        results.append(result)\n",
    "    if not results:\n",
    "        LOGGER.warning(\"No video files found in %s\", source_dir)\n",
    "    return results\n",
    "\n",
    "\n",
    "def build_parser() -> argparse.ArgumentParser:\n",
    "    parser = argparse.ArgumentParser(\n",
    "        description=\"Extract frame sequences from climbing videos.\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"video_dir\",\n",
    "        type=Path,\n",
    "        help=\"Directory containing source video files.\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--output\",\n",
    "        type=Path,\n",
    "        default=Path(\"data\") / \"frames\",\n",
    "        help=\"Directory where frame folders will be stored.\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--interval\",\n",
    "        type=float,\n",
    "        default=1.0,\n",
    "        help=\"Seconds between captured frames (default: 1.0).\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--recursive\",\n",
    "        action=\"store_true\",\n",
    "        help=\"Search for videos recursively.\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--overwrite\",\n",
    "        action=\"store_true\",\n",
    "        help=\"Overwrite existing extracted frames.\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--no-manifest\",\n",
    "        dest=\"write_manifest\",\n",
    "        action=\"store_false\",\n",
    "        help=\"Disable writing manifest.json files.\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--verbose\",\n",
    "        action=\"store_true\",\n",
    "        help=\"Enable verbose logging.\",\n",
    "    )\n",
    "    return parser\n",
    "\n",
    "\n",
    "def main() -> None:\n",
    "    parser = build_parser()\n",
    "    args = parser.parse_args()\n",
    "\n",
    "    configure_logging(verbose=args.verbose)\n",
    "    extract_from_directory(\n",
    "        args.video_dir,\n",
    "        output_root=args.output,\n",
    "        interval_seconds=args.interval,\n",
    "        recursive=args.recursive,\n",
    "        overwrite=args.overwrite,\n",
    "        write_manifest=args.write_manifest,\n",
    "    )\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## scripts/run_feature_export.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"CLI to export pose-derived features from manifests.\"\"\"\n",
    "\n",
    "from __future__ import annotations\n",
    "\n",
    "import argparse\n",
    "import json\n",
    "from pathlib import Path\n",
    "\n",
    "from pose_ai.service import export_features_for_manifest\n",
    "\n",
    "\n",
    "def build_parser() -> argparse.ArgumentParser:\n",
    "    parser = argparse.ArgumentParser(description=\"Export pose feature rows from pose_results.json.\")\n",
    "    parser.add_argument(\"manifest\", type=Path, help=\"Path to manifest.json\")\n",
    "    parser.add_argument(\n",
    "        \"--holds\",\n",
    "        type=Path,\n",
    "        help=\"Optional JSON describing holds (name -> coords, normalized, etc).\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--out\",\n",
    "        type=Path,\n",
    "        default=None,\n",
    "        help=\"Output directory (defaults to manifest directory).\",\n",
    "    )\n",
    "    return parser\n",
    "\n",
    "\n",
    "def main() -> None:\n",
    "    parser = build_parser()\n",
    "    args = parser.parse_args()\n",
    "    output_path = export_features_for_manifest(\n",
    "        args.manifest,\n",
    "        holds_path=args.holds,\n",
    "        output_root=args.out,\n",
    "    )\n",
    "    print(f\"Feature rows saved to {output_path}\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## scripts/run_pipeline.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ae40265",
   "metadata": {},
   "source": [
    "## scripts/generate_holds_and_features.py\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e90f627",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Generate holds.json and enriched pose_features.json for an existing frame directory.\n",
    "\n",
    "Usage:\n",
    "    python scripts/generate_holds_and_features.py /path/to/frames/manifest.json --model yolov8m.pt\n",
    "\"\"\"\n",
    "\n",
    "from __future__ import annotations\n",
    "\n",
    "import argparse\n",
    "from pathlib import Path\n",
    "import sys\n",
    "\n",
    "ROOT_DIR = Path(__file__).resolve().parents[1]\n",
    "SRC_DIR = ROOT_DIR / \"src\"\n",
    "if str(SRC_DIR) not in sys.path:\n",
    "    sys.path.insert(0, str(SRC_DIR))\n",
    "\n",
    "from pose_ai.service.feature_service import export_features_for_manifest\n",
    "from pose_ai.service.hold_extraction import extract_and_cluster_holds, export_holds_json\n",
    "\n",
    "\n",
    "def build_parser() -> argparse.ArgumentParser:\n",
    "    p = argparse.ArgumentParser(description=\"Hold extraction + enriched feature export\")\n",
    "    p.add_argument(\"manifest\", type=Path, help=\"Path to manifest.json\")\n",
    "    p.add_argument(\"--model\", default=\"yolov8n.pt\", help=\"YOLO model weights\")\n",
    "    p.add_argument(\"--device\", default=None, help=\"Optional torch device\")\n",
    "    return p\n",
    "\n",
    "\n",
    "def main() -> None:\n",
    "    parser = build_parser()\n",
    "    args = parser.parse_args()\n",
    "    manifest_path = args.manifest.expanduser().resolve()\n",
    "    if not manifest_path.exists():\n",
    "        raise SystemExit(f\"Manifest not found: {manifest_path}\")\n",
    "    frame_dir = manifest_path.parent\n",
    "    image_paths = sorted(frame_dir.glob(\"*.jpg\"))\n",
    "    if not image_paths:\n",
    "        print(\"No frame images found; aborting hold extraction.\")\n",
    "        holds_path = None\n",
    "    else:\n",
    "        print(f\"Detecting holds in {len(image_paths)} frames using {args.model}\")\n",
    "        clustered = extract_and_cluster_holds(image_paths, model_name=args.model, device=args.device)\n",
    "        if clustered:\n",
    "            holds_path = export_holds_json(clustered, output_path=frame_dir / \"holds.json\")\n",
    "            print(f\"Exported {len(clustered)} clustered holds to {holds_path}\")\n",
    "        else:\n",
    "            holds_path = None\n",
    "            print(\"No holds detected.\")\n",
    "    print(\"Exporting enriched pose features (with wall angle & holds)...\")\n",
    "    export_features_for_manifest(manifest_path, holds_path=holds_path, auto_wall_angle=True)\n",
    "    print(\"Done.\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"End-to-end pipeline: extract frames, run pose estimation, features, segments, visualize.\"\"\"\n",
    "\n",
    "from __future__ import annotations\n",
    "\n",
    "import argparse\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "ROOT_DIR = Path(__file__).resolve().parents[1]\n",
    "SRC_DIR = ROOT_DIR / \"src\"\n",
    "if str(SRC_DIR) not in sys.path:\n",
    "    sys.path.insert(0, str(SRC_DIR))\n",
    "\n",
    "from pose_ai.service import (  # type: ignore  # pylint: disable=wrong-import-position\n",
    "    estimate_poses_from_manifest,\n",
    "    export_features_for_manifest,\n",
    "    generate_segment_report,\n",
    ")\n",
    "from extract_frames import extract_frames_every_n_seconds, iter_video_files  # type: ignore\n",
    "from visualize_pose import visualize_pose_results  # type: ignore\n",
    "\n",
    "# NOTE: For simplicity we call into script helpers directly for pose/feature export\n",
    "# and reuse service APIs for intermediate steps.\n",
    "\n",
    "def build_parser() -> argparse.ArgumentParser:\n",
    "    parser = argparse.ArgumentParser(description=\"Run entire pose analysis pipeline\")\n",
    "    parser.add_argument(\"video_dir\", type=Path, help=\"Directory containing videos (.mp4, etc.)\")\n",
    "    parser.add_argument(\"--out\", type=Path, default=Path(\"data/frames\"), help=\"Frame output directory\")\n",
    "    parser.add_argument(\"--interval\", type=float, default=1.0, help=\"Extraction interval (seconds)\")\n",
    "    parser.add_argument(\"--skip-visuals\", action=\"store_true\", help=\"Skip visualization step\")\n",
    "    return parser\n",
    "\n",
    "\n",
    "def extract_frames(video_dir: Path, out_dir: Path, interval: float) -> list[Path]:\n",
    "    manifests = []\n",
    "    out_dir.mkdir(parents=True, exist_ok=True)\n",
    "    for video_file in iter_video_files(video_dir):\n",
    "        result = extract_frames_every_n_seconds(video_file, output_root=out_dir, interval_seconds=interval)\n",
    "        manifests.append(result.frame_directory / \"manifest.json\")\n",
    "    return manifests\n",
    "\n",
    "\n",
    "def main() -> None:\n",
    "    parser = build_parser()\n",
    "    args = parser.parse_args()\n",
    "    manifests = extract_frames(args.video_dir, args.out, args.interval)\n",
    "    for manifest in manifests:\n",
    "        print(f\"Processing manifest {manifest}\")\n",
    "        estimate_poses_from_manifest(manifest)\n",
    "        export_features_for_manifest(manifest)\n",
    "        generate_segment_report(manifest)\n",
    "        if not args.skip_visuals:\n",
    "            frame_dir = manifest.parent\n",
    "            visualize_pose_results(frame_dir / \"pose_results.json\")\n",
    "    print(\"Pipeline completed.\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## scripts/run_pose_estimation.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"CLI to run pose estimation on extracted frame sequences.\"\"\"\n",
    "\n",
    "from __future__ import annotations\n",
    "\n",
    "import argparse\n",
    "import json\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "from pose_ai.pose import PoseEstimator\n",
    "from pose_ai.service import estimate_poses_for_directory, estimate_poses_from_manifest\n",
    "\n",
    "\n",
    "def build_parser() -> argparse.ArgumentParser:\n",
    "    parser = argparse.ArgumentParser(description=\"Run MediaPipe pose estimation on frame manifests.\")\n",
    "    group = parser.add_mutually_exclusive_group(required=True)\n",
    "    group.add_argument(\"--manifest\", type=Path, help=\"Path to a manifest.json file.\")\n",
    "    group.add_argument(\"--frames-root\", type=Path, help=\"Directory containing extracted frame folders.\")\n",
    "    parser.add_argument(\n",
    "        \"--json\",\n",
    "        action=\"store_true\",\n",
    "        help=\"Print pose results as JSON instead of a textual summary.\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--no-save\",\n",
    "        action=\"store_true\",\n",
    "        help=\"Disable writing pose_results.json files alongside frames.\",\n",
    "    )\n",
    "    return parser\n",
    "\n",
    "\n",
    "def frames_to_dict(frames):\n",
    "    return [\n",
    "        {\n",
    "            \"image_path\": str(frame.image_path),\n",
    "            \"timestamp_seconds\": frame.timestamp_seconds,\n",
    "            \"detection_score\": frame.detection_score,\n",
    "            \"landmarks\": [\n",
    "                {\n",
    "                    \"name\": landmark.name,\n",
    "                    \"x\": landmark.x,\n",
    "                    \"y\": landmark.y,\n",
    "                    \"z\": landmark.z,\n",
    "                    \"visibility\": landmark.visibility,\n",
    "                }\n",
    "                for landmark in frame.landmarks\n",
    "            ],\n",
    "        }\n",
    "        for frame in frames\n",
    "    ]\n",
    "\n",
    "\n",
    "def main() -> None:\n",
    "    parser = build_parser()\n",
    "    args = parser.parse_args()\n",
    "\n",
    "    estimator = PoseEstimator()\n",
    "    try:\n",
    "        if args.manifest:\n",
    "            frames = estimate_poses_from_manifest(\n",
    "                args.manifest,\n",
    "                estimator=estimator,\n",
    "                save_json=not args.no_save,\n",
    "            )\n",
    "            if args.json:\n",
    "                print(json.dumps(frames_to_dict(frames), indent=2))\n",
    "            else:\n",
    "                print(f\"Processed {len(frames)} frames from {args.manifest}\")\n",
    "        else:\n",
    "            results = estimate_poses_for_directory(\n",
    "                args.frames_root,\n",
    "                estimator=estimator,\n",
    "                save_json=not args.no_save,\n",
    "            )\n",
    "            if args.json:\n",
    "                payload = {manifest: frames_to_dict(frames) for manifest, frames in results.items()}\n",
    "                print(json.dumps(payload, indent=2))\n",
    "            else:\n",
    "                for manifest, frames in results.items():\n",
    "                    print(f\"{manifest}: {len(frames)} frames\")\n",
    "    except ModuleNotFoundError as exc:\n",
    "        parser.error(\n",
    "            f\"{exc}. Ensure mediapipe is installed in your environment (e.g. `pip install mediapipe`).\"\n",
    "        )\n",
    "    finally:\n",
    "        estimator.close()\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57b9dd4a",
   "metadata": {},
   "source": [
    "## scripts/train_model.py\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ebce044",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python3\n",
    "\"\"\"Train multitask model (BiLSTM or Transformer).\n",
    "\n",
    "Usage:\n",
    "    # Train BiLSTM (default)\n",
    "    python scripts/train_model.py \\\n",
    "        --data data/features \\\n",
    "        --epochs 100 \\\n",
    "        --device cuda\n",
    "\n",
    "    # Train Transformer\n",
    "    python scripts/train_model.py \\\n",
    "        --data data/features \\\n",
    "        --model-type transformer \\\n",
    "        --num-layers 4 \\\n",
    "        --num-heads 8 \\\n",
    "        --epochs 100 \\\n",
    "        --device cuda\n",
    "\"\"\"\n",
    "\n",
    "import argparse\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "# Add src to path\n",
    "sys.path.insert(0, str(Path(__file__).parent.parent / \"src\"))\n",
    "\n",
    "try:\n",
    "    import torch\n",
    "    from torch.utils.data import DataLoader\n",
    "except ImportError:\n",
    "    print(\"Error: PyTorch is required. Install with: pip install torch\")\n",
    "    sys.exit(1)\n",
    "\n",
    "from pose_ai.ml.dataset import create_datasets_from_directory\n",
    "from pose_ai.ml.models import (\n",
    "    BiLSTMMultitaskModel,\n",
    "    TransformerMultitaskModel,\n",
    "    ModelConfig,\n",
    "    TransformerConfig,\n",
    "    count_parameters,\n",
    ")\n",
    "from pose_ai.ml.train import Trainer, TrainingConfig\n",
    "\n",
    "\n",
    "def main():\n",
    "    parser = argparse.ArgumentParser(description=\"Train multitask model (BiLSTM or Transformer)\")\n",
    "    \n",
    "    # Model selection\n",
    "    parser.add_argument(\n",
    "        \"--model-type\",\n",
    "        choices=[\"bilstm\", \"transformer\"],\n",
    "        default=\"bilstm\",\n",
    "        help=\"Model architecture type (default: bilstm)\"\n",
    "    )\n",
    "    \n",
    "    # Data and checkpointing\n",
    "    parser.add_argument(\n",
    "        \"--data\",\n",
    "        type=Path,\n",
    "        required=True,\n",
    "        help=\"Directory containing feature JSON files\"\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--checkpoint-dir\",\n",
    "        type=Path,\n",
    "        default=Path(\"models/checkpoints\"),\n",
    "        help=\"Directory for model checkpoints (default: models/checkpoints)\"\n",
    "    )\n",
    "    \n",
    "    # Training hyperparameters\n",
    "    parser.add_argument(\"--epochs\", type=int, default=100, help=\"Number of training epochs (default: 100)\")\n",
    "    parser.add_argument(\"--batch-size\", type=int, default=32, help=\"Batch size (default: 32)\")\n",
    "    parser.add_argument(\"--window-size\", type=int, default=32, help=\"Window size for sliding windows (default: 32)\")\n",
    "    parser.add_argument(\"--stride\", type=int, default=1, help=\"Stride for sliding windows (default: 1)\")\n",
    "    parser.add_argument(\"--dropout\", type=float, default=0.3, help=\"Dropout rate (default: 0.3)\")\n",
    "    parser.add_argument(\"--lr\", type=float, default=0.001, help=\"Learning rate (default: 0.001)\")\n",
    "    parser.add_argument(\"--weight-decay\", type=float, default=0.0001, help=\"Weight decay (default: 0.0001)\")\n",
    "    parser.add_argument(\"--patience\", type=int, default=10, help=\"Early stopping patience (default: 10)\")\n",
    "    parser.add_argument(\"--device\", default=\"cuda\", help=\"Device to use (cuda/cpu, default: cuda)\")\n",
    "    parser.add_argument(\"--train-split\", type=float, default=0.7, help=\"Training split ratio (default: 0.7)\")\n",
    "    parser.add_argument(\"--val-split\", type=float, default=0.2, help=\"Validation split ratio (default: 0.2)\")\n",
    "    \n",
    "    # BiLSTM-specific arguments\n",
    "    parser.add_argument(\"--hidden-dim\", type=int, default=128, help=\"LSTM hidden dimension (default: 128)\")\n",
    "    parser.add_argument(\"--num-layers\", type=int, default=2, help=\"Number of LSTM/Transformer layers (default: 2 for BiLSTM, 4 for Transformer)\")\n",
    "    parser.add_argument(\"--no-attention\", action=\"store_true\", help=\"[BiLSTM only] Disable attention pooling\")\n",
    "    \n",
    "    # Transformer-specific arguments\n",
    "    parser.add_argument(\"--d-model\", type=int, default=128, help=\"[Transformer only] Model dimension (default: 128)\")\n",
    "    parser.add_argument(\"--num-heads\", type=int, default=8, help=\"[Transformer only] Number of attention heads (default: 8)\")\n",
    "    parser.add_argument(\"--dim-feedforward\", type=int, default=512, help=\"[Transformer only] Feedforward network dimension (default: 512)\")\n",
    "    parser.add_argument(\"--pooling\", choices=[\"mean\", \"max\", \"cls\"], default=\"mean\", help=\"[Transformer only] Pooling strategy (default: mean)\")\n",
    "    parser.add_argument(\"--positional-encoding\", choices=[\"sinusoidal\", \"learnable\"], default=\"sinusoidal\", help=\"[Transformer only] Positional encoding type (default: sinusoidal)\")\n",
    "    \n",
    "    args = parser.parse_args()\n",
    "    \n",
    "    # Validate data directory\n",
    "    if not args.data.exists():\n",
    "        print(f\"Error: Data directory not found: {args.data}\")\n",
    "        sys.exit(1)\n",
    "    \n",
    "    # Create checkpoint directory\n",
    "    args.checkpoint_dir.mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    print(\"=\" * 60)\n",
    "    print(f\"{args.model_type.upper()} Multitask Model Training\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    # Create datasets\n",
    "    print(\"\\nCreating datasets...\")\n",
    "    train_dataset, val_dataset, test_dataset = create_datasets_from_directory(\n",
    "        args.data,\n",
    "        window_size=args.window_size,\n",
    "        stride=args.stride,\n",
    "        train_split=args.train_split,\n",
    "        val_split=args.val_split,\n",
    "        normalize=True,\n",
    "    )\n",
    "    \n",
    "    if train_dataset is None or val_dataset is None:\n",
    "        print(\"Error: Failed to create datasets\")\n",
    "        sys.exit(1)\n",
    "    \n",
    "    print(f\"  Training samples: {len(train_dataset)}\")\n",
    "    print(f\"  Validation samples: {len(val_dataset)}\")\n",
    "    if test_dataset:\n",
    "        print(f\"  Test samples: {len(test_dataset)}\")\n",
    "    \n",
    "    # Create data loaders\n",
    "    train_loader = DataLoader(train_dataset, batch_size=args.batch_size, shuffle=True, num_workers=4, pin_memory=True if args.device == \"cuda\" else False)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=args.batch_size, shuffle=False, num_workers=4, pin_memory=True if args.device == \"cuda\" else False)\n",
    "    \n",
    "    # Create model based on type\n",
    "    print(\"\\nCreating model...\")\n",
    "    \n",
    "    if args.model_type == \"bilstm\":\n",
    "        model_config = ModelConfig(\n",
    "            input_dim=60,\n",
    "            hidden_dim=args.hidden_dim,\n",
    "            num_layers=args.num_layers,\n",
    "            dropout=args.dropout,\n",
    "            num_action_classes=5,\n",
    "            bidirectional=True,\n",
    "            use_attention=not args.no_attention,\n",
    "        )\n",
    "        model = BiLSTMMultitaskModel(model_config)\n",
    "        print(f\"  Model: BiLSTM (bidirectional={model_config.bidirectional})\")\n",
    "        print(f\"  Hidden dim: {args.hidden_dim}\")\n",
    "        print(f\"  Layers: {args.num_layers}\")\n",
    "        print(f\"  Attention: {model_config.use_attention}\")\n",
    "    elif args.model_type == \"transformer\":\n",
    "        num_layers = args.num_layers if args.num_layers != 2 else 4\n",
    "        transformer_config = TransformerConfig(\n",
    "            input_dim=60,\n",
    "            d_model=args.d_model,\n",
    "            nhead=args.num_heads,\n",
    "            num_layers=num_layers,\n",
    "            dim_feedforward=args.dim_feedforward,\n",
    "            dropout=args.dropout,\n",
    "            num_action_classes=5,\n",
    "            pooling=args.pooling,\n",
    "            positional_encoding=args.positional_encoding,\n",
    "        )\n",
    "        model = TransformerMultitaskModel(transformer_config)\n",
    "        print(f\"  Model: Transformer\")\n",
    "        print(f\"  d_model: {args.d_model}\")\n",
    "        print(f\"  Layers: {num_layers}\")\n",
    "        print(f\"  Heads: {args.num_heads}\")\n",
    "        print(f\"  Pooling: {args.pooling}\")\n",
    "        print(f\"  Positional encoding: {args.positional_encoding}\")\n",
    "    else:\n",
    "        print(f\"Error: Unknown model type: {args.model_type}\")\n",
    "        sys.exit(1)\n",
    "    \n",
    "    num_params = count_parameters(model)\n",
    "    print(f\"  Total parameters: {num_params:,}\")\n",
    "    \n",
    "    # Create trainer\n",
    "    print(\"\\nSetting up trainer...\")\n",
    "    training_config = TrainingConfig(\n",
    "        epochs=args.epochs,\n",
    "        batch_size=args.batch_size,\n",
    "        learning_rate=args.lr,\n",
    "        weight_decay=args.weight_decay,\n",
    "        patience=args.patience,\n",
    "        device=args.device,\n",
    "        checkpoint_dir=args.checkpoint_dir,\n",
    "        log_interval=10,\n",
    "    )\n",
    "    \n",
    "    trainer = Trainer(model=model, train_loader=train_loader, val_loader=val_loader, config=training_config)\n",
    "    \n",
    "    # Train\n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    trainer.train()\n",
    "    \n",
    "    # Save final model\n",
    "    print(\"\\nSaving final model...\")\n",
    "    final_model_path = args.checkpoint_dir / f\"{args.model_type}_multitask.pt\"\n",
    "    model.save(final_model_path)\n",
    "    print(f\"Model saved to: {final_model_path}\")\n",
    "    \n",
    "    # Save normalization parameters\n",
    "    if train_dataset.feature_mean is not None:\n",
    "        import numpy as np\n",
    "        norm_path = args.checkpoint_dir / \"normalization.npz\"\n",
    "        np.savez(norm_path, mean=train_dataset.feature_mean, std=train_dataset.feature_std)\n",
    "        print(f\"Normalization parameters saved to: {norm_path}\")\n",
    "    \n",
    "    print(\"\\nTraining complete!\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97e0f8d6",
   "metadata": {},
   "source": [
    "## scripts/evaluate_model.py\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd6a1428",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python3\n",
    "\"\"\"Evaluate multitask model (BiLSTM or Transformer).\n",
    "\n",
    "Usage:\n",
    "    python scripts/evaluate_model.py \\\n",
    "        --model models/checkpoints/bilstm_multitask.pt \\\n",
    "        --data data/features \\\n",
    "        --device cuda\n",
    "\"\"\"\n",
    "\n",
    "import argparse\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "# Add src to path\n",
    "sys.path.insert(0, str(Path(__file__).parent.parent / \"src\"))\n",
    "\n",
    "try:\n",
    "    import torch\n",
    "    from torch.utils.data import DataLoader\n",
    "    import numpy as np\n",
    "except ImportError:\n",
    "    print(\"Error: PyTorch and NumPy are required. Install with: pip install torch numpy\")\n",
    "    sys.exit(1)\n",
    "\n",
    "from pose_ai.ml.dataset import create_datasets_from_directory\n",
    "from pose_ai.ml.models import BiLSTMMultitaskModel, TransformerMultitaskModel, MultitaskLoss\n",
    "\n",
    "\n",
    "def load_model(model_path: Path, device: str):\n",
    "    \"\"\"Load model with auto-detection of type.\"\"\"\n",
    "    checkpoint = torch.load(model_path, map_location=device)\n",
    "    model_type = checkpoint.get(\"model_type\", \"bilstm\")\n",
    "    print(f\"Detected model type: {model_type}\")\n",
    "    \n",
    "    if model_type == \"bilstm\":\n",
    "        model = BiLSTMMultitaskModel.load(model_path, device=device)\n",
    "    elif model_type == \"transformer\":\n",
    "        model = TransformerMultitaskModel.load(model_path, device=device)\n",
    "    else:\n",
    "        raise ValueError(f\"Unknown model type: {model_type}\")\n",
    "    \n",
    "    return model, model_type\n",
    "\n",
    "\n",
    "def evaluate_model(model, data_loader, criterion, device) -> dict:\n",
    "    \"\"\"Evaluate model on dataset.\"\"\"\n",
    "    model.eval()\n",
    "    total_loss = 0.0\n",
    "    total_eff_loss = 0.0\n",
    "    total_action_loss = 0.0\n",
    "    num_batches = 0\n",
    "    \n",
    "    all_eff_pred = []\n",
    "    all_eff_true = []\n",
    "    all_action_pred = []\n",
    "    all_action_true = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for features, eff_labels, action_labels in data_loader:\n",
    "            features = features.to(device)\n",
    "            eff_labels = eff_labels.to(device)\n",
    "            action_labels = action_labels.to(device)\n",
    "            \n",
    "            eff_pred, action_logits = model(features)\n",
    "            loss, loss_dict = criterion(eff_pred, eff_labels, action_logits, action_labels)\n",
    "            \n",
    "            total_loss += loss_dict[\"total\"]\n",
    "            total_eff_loss += loss_dict[\"efficiency\"]\n",
    "            total_action_loss += loss_dict[\"action\"]\n",
    "            num_batches += 1\n",
    "            \n",
    "            all_eff_pred.extend(eff_pred.cpu().numpy().flatten())\n",
    "            all_eff_true.extend(eff_labels.cpu().numpy())\n",
    "            all_action_pred.extend(action_logits.argmax(dim=-1).cpu().numpy())\n",
    "            all_action_true.extend(action_labels.cpu().numpy())\n",
    "    \n",
    "    all_eff_pred = np.array(all_eff_pred)\n",
    "    all_eff_true = np.array(all_eff_true)\n",
    "    all_action_pred = np.array(all_action_pred)\n",
    "    all_action_true = np.array(all_action_true)\n",
    "    \n",
    "    eff_mae = np.abs(all_eff_pred - all_eff_true).mean()\n",
    "    eff_rmse = np.sqrt(((all_eff_pred - all_eff_true) ** 2).mean())\n",
    "    eff_corr = np.corrcoef(all_eff_pred, all_eff_true)[0, 1] if len(all_eff_pred) > 1 else 0.0\n",
    "    \n",
    "    ss_res = ((all_eff_true - all_eff_pred) ** 2).sum()\n",
    "    ss_tot = ((all_eff_true - all_eff_true.mean()) ** 2).sum()\n",
    "    r2 = 1 - (ss_res / ss_tot) if ss_tot > 0 else 0.0\n",
    "    \n",
    "    action_acc = (all_action_pred == all_action_true).mean()\n",
    "    \n",
    "    per_class_acc = {}\n",
    "    for class_id in range(5):\n",
    "        mask = all_action_true == class_id\n",
    "        if mask.sum() > 0:\n",
    "            per_class_acc[class_id] = (all_action_pred[mask] == all_action_true[mask]).mean()\n",
    "    \n",
    "    return {\n",
    "        \"loss\": {\n",
    "            \"total\": total_loss / num_batches,\n",
    "            \"efficiency\": total_eff_loss / num_batches,\n",
    "            \"action\": total_action_loss / num_batches,\n",
    "        },\n",
    "        \"efficiency\": {\n",
    "            \"mae\": float(eff_mae),\n",
    "            \"rmse\": float(eff_rmse),\n",
    "            \"correlation\": float(eff_corr),\n",
    "            \"r2\": float(r2),\n",
    "        },\n",
    "        \"action\": {\n",
    "            \"accuracy\": float(action_acc),\n",
    "            \"per_class\": {k: float(v) for k, v in per_class_acc.items()},\n",
    "        },\n",
    "    }\n",
    "\n",
    "\n",
    "def main():\n",
    "    parser = argparse.ArgumentParser(description=\"Evaluate multitask model (BiLSTM or Transformer)\")\n",
    "    parser.add_argument(\"--model\", type=Path, required=True, help=\"Path to trained model (.pt file)\")\n",
    "    parser.add_argument(\"--data\", type=Path, required=True, help=\"Directory containing feature JSON files\")\n",
    "    parser.add_argument(\"--batch-size\", type=int, default=32, help=\"Batch size (default: 32)\")\n",
    "    parser.add_argument(\"--device\", default=\"cuda\", help=\"Device to use (cuda/cpu, default: cuda)\")\n",
    "    parser.add_argument(\"--split\", default=\"test\", choices=[\"train\", \"val\", \"test\", \"all\"], help=\"Which split to evaluate (default: test)\")\n",
    "    parser.add_argument(\"--window-size\", type=int, default=32, help=\"Window size (default: 32)\")\n",
    "    \n",
    "    args = parser.parse_args()\n",
    "    \n",
    "    if not args.model.exists():\n",
    "        print(f\"Error: Model file not found: {args.model}\")\n",
    "        sys.exit(1)\n",
    "    \n",
    "    if not args.data.exists():\n",
    "        print(f\"Error: Data directory not found: {args.data}\")\n",
    "        sys.exit(1)\n",
    "    \n",
    "    if args.device == \"cuda\" and not torch.cuda.is_available():\n",
    "        print(\"CUDA not available, using CPU\")\n",
    "        args.device = \"cpu\"\n",
    "    device = torch.device(args.device)\n",
    "    \n",
    "    print(\"=\" * 60)\n",
    "    print(\"Model Evaluation\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    print(f\"\\nLoading model from: {args.model}\")\n",
    "    model, model_type = load_model(args.model, args.device)\n",
    "    print(f\"Model type: {model_type}\")\n",
    "    print(f\"Model loaded on: {args.device}\")\n",
    "    \n",
    "    print(\"\\nLoading datasets...\")\n",
    "    train_dataset, val_dataset, test_dataset = create_datasets_from_directory(\n",
    "        args.data, window_size=args.window_size, stride=1, normalize=True\n",
    "    )\n",
    "    \n",
    "    criterion = MultitaskLoss(efficiency_weight=1.0, action_weight=0.5, efficiency_loss=\"huber\")\n",
    "    \n",
    "    if args.split == \"all\":\n",
    "        splits = [(\"Train\", train_dataset), (\"Val\", val_dataset), (\"Test\", test_dataset)]\n",
    "    elif args.split == \"train\":\n",
    "        splits = [(\"Train\", train_dataset)]\n",
    "    elif args.split == \"val\":\n",
    "        splits = [(\"Val\", val_dataset)]\n",
    "    else:\n",
    "        splits = [(\"Test\", test_dataset)]\n",
    "    \n",
    "    for split_name, dataset in splits:\n",
    "        if dataset is None:\n",
    "            print(f\"\\n{split_name} dataset not available\")\n",
    "            continue\n",
    "        \n",
    "        print(f\"\\nEvaluating on {split_name} set ({len(dataset)} samples)...\")\n",
    "        data_loader = DataLoader(dataset, batch_size=args.batch_size, shuffle=False, num_workers=4)\n",
    "        results = evaluate_model(model, data_loader, criterion, device)\n",
    "        \n",
    "        print(f\"\\n{split_name} Results:\")\n",
    "        print(\"=\" * 60)\n",
    "        print(\"\\nLoss:\")\n",
    "        print(f\"  Total: {results['loss']['total']:.4f}\")\n",
    "        print(f\"  Efficiency: {results['loss']['efficiency']:.4f}\")\n",
    "        print(f\"  Action: {results['loss']['action']:.4f}\")\n",
    "        print(\"\\nEfficiency Regression:\")\n",
    "        print(f\"  MAE: {results['efficiency']['mae']:.4f}\")\n",
    "        print(f\"  RMSE: {results['efficiency']['rmse']:.4f}\")\n",
    "        print(f\"  Correlation: {results['efficiency']['correlation']:.4f}\")\n",
    "        print(f\"  R²: {results['efficiency']['r2']:.4f}\")\n",
    "        print(\"\\nNext-Action Classification:\")\n",
    "        print(f\"  Accuracy: {results['action']['accuracy']:.4f}\")\n",
    "        print(\"\\n  Per-Class Accuracy:\")\n",
    "        class_names = [\"no_change\", \"left_hand\", \"right_hand\", \"left_foot\", \"right_foot\"]\n",
    "        for class_id, acc in results['action']['per_class'].items():\n",
    "            print(f\"    {class_names[class_id]}: {acc:.4f}\")\n",
    "    \n",
    "    print(\"\\nEvaluation complete!\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## scripts/run_segment_report.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"CLI to generate segment-level metrics.\"\"\"\n",
    "\n",
    "from __future__ import annotations\n",
    "\n",
    "import argparse\n",
    "\n",
    "from pose_ai.service import generate_segment_report\n",
    "\n",
    "\n",
    "def build_parser() -> argparse.ArgumentParser:\n",
    "    parser = argparse.ArgumentParser(description=\"Export segment metrics (COM, joints, contacts)\")\n",
    "    parser.add_argument(\"manifest\", type=str, help=\"Path to manifest.json\")\n",
    "    parser.add_argument(\"--holds\", type=str, help=\"Optional holds JSON path\", default=None)\n",
    "    return parser\n",
    "\n",
    "\n",
    "def main() -> None:\n",
    "    parser = build_parser()\n",
    "    args = parser.parse_args()\n",
    "    metrics = generate_segment_report(args.manifest, holds_path=Path(args.holds) if args.holds else None)\n",
    "    print(f\"Saved {len(metrics)} segments\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## scripts/run_segmentation.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"CLI to run rule-based segmentation over extracted frame manifests.\"\"\"\n",
    "\n",
    "from __future__ import annotations\n",
    "\n",
    "import argparse\n",
    "import json\n",
    "from pathlib import Path\n",
    "\n",
    "from pose_ai.service import segment_video_from_manifest, segment_videos_under_directory\n",
    "\n",
    "\n",
    "def build_parser() -> argparse.ArgumentParser:\n",
    "    parser = argparse.ArgumentParser(description=\"Produce rest/movement segments from frame manifests.\")\n",
    "    group = parser.add_mutually_exclusive_group(required=True)\n",
    "    group.add_argument(\"--manifest\", type=Path, help=\"Path to a manifest.json file.\")\n",
    "    group.add_argument(\"--frames-root\", type=Path, help=\"Directory containing subfolders with manifest.json files.\")\n",
    "    parser.add_argument(\"--json\", action=\"store_true\", help=\"Print segmentation results as JSON.\")\n",
    "    return parser\n",
    "\n",
    "\n",
    "def _segment_to_dict(segment):\n",
    "    return {\n",
    "        \"start_time\": segment.start_time,\n",
    "        \"end_time\": segment.end_time,\n",
    "        \"label\": segment.label,\n",
    "        \"duration\": segment.duration,\n",
    "        \"frame_indices\": segment.frame_indices,\n",
    "    }\n",
    "\n",
    "\n",
    "def main() -> None:\n",
    "    parser = build_parser()\n",
    "    args = parser.parse_args()\n",
    "\n",
    "    if args.manifest:\n",
    "        segments = segment_video_from_manifest(args.manifest)\n",
    "        if args.json:\n",
    "            print(json.dumps([_segment_to_dict(seg) for seg in segments], indent=2))\n",
    "        else:\n",
    "            print(f\"Segments for {args.manifest}:\")\n",
    "            for seg in segments:\n",
    "                print(\n",
    "                    f\"- {seg.label:9s} {seg.start_time:5.2f}s → {seg.end_time:5.2f}s \"\n",
    "                    f\"(duration {seg.duration:4.2f}s, frames {seg.frame_indices})\"\n",
    "                )\n",
    "    else:\n",
    "        results = segment_videos_under_directory(args.frames_root)\n",
    "        if args.json:\n",
    "            payload = {\n",
    "                manifest: [_segment_to_dict(seg) for seg in segments]\n",
    "                for manifest, segments in results.items()\n",
    "            }\n",
    "            print(json.dumps(payload, indent=2))\n",
    "        else:\n",
    "            for manifest, segments in results.items():\n",
    "                print(f\"Segments for {manifest}:\")\n",
    "                for seg in segments:\n",
    "                    print(\n",
    "                        f\"  - {seg.label:9s} {seg.start_time:5.2f}s → {seg.end_time:5.2f}s \"\n",
    "                        f\"(duration {seg.duration:4.2f}s, frames {seg.frame_indices})\"\n",
    "                    )\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b4a4d64",
   "metadata": {},
   "source": [
    "## Advanced Efficiency and Planner Modules\n",
    "\n",
    "These modules provide advanced efficiency scoring and next-move planning capabilities.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "874fe1dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# ADVANCED PLANNER MODULE\n",
    "# ============================================================================\n",
    "# Rule-based next-move planner with efficiency simulation\n",
    "# ============================================================================\n",
    "\n",
    "from __future__ import annotations\n",
    "from dataclasses import dataclass\n",
    "from typing import Dict, List, MutableMapping, Optional, Sequence, Tuple\n",
    "import math\n",
    "\n",
    "try:\n",
    "    import numpy as np\n",
    "    HAS_NUMPY = True\n",
    "except ImportError:\n",
    "    HAS_NUMPY = False\n",
    "    np = None\n",
    "\n",
    "Point = Tuple[float, float]\n",
    "\n",
    "\n",
    "@dataclass(slots=True)\n",
    "class MoveCandidate:\n",
    "    \"\"\"Represents a candidate next move.\"\"\"\n",
    "    limb: str\n",
    "    hold_id: str\n",
    "    hold_position: Point\n",
    "    simulated_efficiency: float\n",
    "    efficiency_delta: float\n",
    "    reasoning: str\n",
    "    constraint_violations: List[str]\n",
    "    hold_type: str | None = None\n",
    "\n",
    "\n",
    "@dataclass(slots=True)\n",
    "class PlannerConfig:\n",
    "    \"\"\"Configuration for the rule-based planner.\"\"\"\n",
    "    k_candidates: int = 10\n",
    "    upward_bias: float = 0.3\n",
    "    min_support_count: int = 2\n",
    "    max_reach_ratio: float = 1.2\n",
    "    com_polygon_tolerance: float = 0.15\n",
    "    stability_alpha: float = 4.0\n",
    "    prefer_jug_when_low_support: bool = True\n",
    "    jug_bonus: float = 0.05\n",
    "    reach_hold_bonus: float = 0.03\n",
    "    \n",
    "    def get_adjusted_reach_ratio(\n",
    "        self,\n",
    "        climber_wingspan: float | None = None,\n",
    "        climber_height: float | None = None,\n",
    "        climber_flexibility: float | None = None,\n",
    "    ) -> float:\n",
    "        \"\"\"Compute personalized reach ratio based on climber attributes.\"\"\"\n",
    "        adjusted_ratio = self.max_reach_ratio\n",
    "        if climber_wingspan is not None and climber_height is not None and climber_height > 0:\n",
    "            wingspan_ratio = climber_wingspan / climber_height\n",
    "            wingspan_multiplier = min(1.2, max(0.8, 0.9 + 0.2 * wingspan_ratio))\n",
    "            adjusted_ratio *= wingspan_multiplier\n",
    "        if climber_flexibility is not None:\n",
    "            flexibility_bonus = 0.05 + 0.05 * climber_flexibility\n",
    "            adjusted_ratio *= (1.0 + flexibility_bonus)\n",
    "        return adjusted_ratio\n",
    "\n",
    "\n",
    "def _safe_float(value) -> float | None:\n",
    "    if value is None:\n",
    "        return None\n",
    "    try:\n",
    "        return float(value)\n",
    "    except Exception:\n",
    "        return None\n",
    "\n",
    "\n",
    "def _distance(p1: Point, p2: Point) -> float:\n",
    "    return math.sqrt((p1[0] - p2[0]) ** 2 + (p1[1] - p2[1]) ** 2)\n",
    "\n",
    "\n",
    "def _point_in_polygon(point: Point, polygon: Sequence[Point]) -> bool:\n",
    "    \"\"\"Ray casting algorithm for point-in-polygon test.\"\"\"\n",
    "    if len(polygon) < 3:\n",
    "        return False\n",
    "    x, y = point\n",
    "    inside = False\n",
    "    for i in range(len(polygon)):\n",
    "        x1, y1 = polygon[i]\n",
    "        x2, y2 = polygon[(i + 1) % len(polygon)]\n",
    "        cond = ((y1 > y) != (y2 > y)) and (x < (x2 - x1) * (y - y1) / ((y2 - y1) or 1e-6) + x1)\n",
    "        if cond:\n",
    "            inside = not inside\n",
    "    return inside\n",
    "\n",
    "\n",
    "def _distance_to_polygon(point: Point, polygon: Sequence[Point]) -> float:\n",
    "    \"\"\"Distance from point to polygon.\"\"\"\n",
    "    if not polygon:\n",
    "        return float(\"inf\")\n",
    "    if len(polygon) == 1:\n",
    "        return _distance(point, polygon[0])\n",
    "    if _point_in_polygon(point, polygon):\n",
    "        return 0.0\n",
    "    min_dist = float(\"inf\")\n",
    "    for i in range(len(polygon)):\n",
    "        a = polygon[i]\n",
    "        b = polygon[(i + 1) % len(polygon)]\n",
    "        ax, ay = a\n",
    "        bx, by = b\n",
    "        px, py = point\n",
    "        abx = bx - ax\n",
    "        aby = by - ay\n",
    "        if abs(abx) < 1e-9 and abs(aby) < 1e-9:\n",
    "            dist = _distance(point, a)\n",
    "        else:\n",
    "            t = max(0.0, min(1.0, ((px - ax) * abx + (py - ay) * aby) / (abx * abx + aby * aby)))\n",
    "            proj_x = ax + t * abx\n",
    "            proj_y = ay + t * aby\n",
    "            dist = math.sqrt((px - proj_x) ** 2 + (py - proj_y) ** 2)\n",
    "        min_dist = min(min_dist, dist)\n",
    "    return min_dist\n",
    "\n",
    "\n",
    "def _compute_stability_score(com: Point, support_points: List[Point], body_scale: float, alpha: float = 4.0) -> float:\n",
    "    \"\"\"Compute support polygon stability score.\"\"\"\n",
    "    if not support_points:\n",
    "        return 0.0\n",
    "    distance = _distance_to_polygon(com, support_points)\n",
    "    return math.exp(-alpha * (distance / (body_scale + 1e-6)))\n",
    "\n",
    "\n",
    "class NextMovePlanner:\n",
    "    \"\"\"Rule-based planner for next move recommendations.\"\"\"\n",
    "    \n",
    "    def __init__(self, config: Optional[PlannerConfig] = None):\n",
    "        self.config = config or PlannerConfig()\n",
    "    \n",
    "    def plan_next_move(\n",
    "        self,\n",
    "        current_row: MutableMapping[str, object],\n",
    "        holds: Sequence[Dict[str, object]],\n",
    "        *,\n",
    "        top_k: int = 3,\n",
    "    ) -> List[MoveCandidate]:\n",
    "        \"\"\"Generate ranked next-move candidates with efficiency simulation.\"\"\"\n",
    "        if not holds:\n",
    "            return []\n",
    "        \n",
    "        com_x = _safe_float(current_row.get(\"com_x\"))\n",
    "        com_y = _safe_float(current_row.get(\"com_y\"))\n",
    "        body_scale = _safe_float(current_row.get(\"body_scale\")) or 1.0\n",
    "        \n",
    "        if com_x is None or com_y is None:\n",
    "            return []\n",
    "        \n",
    "        com = (com_x, com_y)\n",
    "        used_holds = set()\n",
    "        for limb in (\"left_hand\", \"right_hand\", \"left_foot\", \"right_foot\"):\n",
    "            hold_id = current_row.get(f\"{limb}_contact_hold\")\n",
    "            if hold_id:\n",
    "                used_holds.add(str(hold_id))\n",
    "        \n",
    "        candidates: List[Tuple[str, str, Point, float, str | None]] = []\n",
    "        \n",
    "        for hold in holds:\n",
    "            hold_id = str(hold.get(\"hold_id\") or hold.get(\"name\", \"\"))\n",
    "            if hold_id in used_holds:\n",
    "                continue\n",
    "            \n",
    "            coords = hold.get(\"coords\")\n",
    "            if not isinstance(coords, (list, tuple)) or len(coords) < 2:\n",
    "                continue\n",
    "            \n",
    "            hx, hy = float(coords[0]), float(coords[1])\n",
    "            hold_pos = (hx, hy)\n",
    "            hold_type = hold.get(\"hold_type\")\n",
    "            \n",
    "            dist = _distance(com, hold_pos)\n",
    "            dist_score = 1.0 / (dist / body_scale + 1e-6)\n",
    "            \n",
    "            upward_score = 0.0\n",
    "            if hy < com_y:\n",
    "                upward_score = self.config.upward_bias * (com_y - hy)\n",
    "            \n",
    "            type_bonus = 0.0\n",
    "            if hold_type:\n",
    "                current_support = sum(1 for limb in (\"left_hand\", \"right_hand\", \"left_foot\", \"right_foot\") \n",
    "                                    if current_row.get(f\"{limb}_contact_on\"))\n",
    "                if self.config.prefer_jug_when_low_support and hold_type == \"jug\" and current_support < 3:\n",
    "                    type_bonus = self.config.jug_bonus\n",
    "                if hy < com_y - 0.15 * body_scale and hold_type in (\"crimp\", \"sloper\"):\n",
    "                    type_bonus = max(type_bonus, self.config.reach_hold_bonus)\n",
    "            \n",
    "            sample_score = dist_score + upward_score + type_bonus\n",
    "            \n",
    "            if hy < com_y - 0.1 * body_scale:\n",
    "                for limb in (\"left_hand\", \"right_hand\"):\n",
    "                    if not current_row.get(f\"{limb}_contact_on\"):\n",
    "                        candidates.append((limb, hold_id, hold_pos, sample_score, hold_type))\n",
    "                        break\n",
    "            else:\n",
    "                for limb in (\"left_foot\", \"right_foot\"):\n",
    "                    if not current_row.get(f\"{limb}_contact_on\"):\n",
    "                        candidates.append((limb, hold_id, hold_pos, sample_score, hold_type))\n",
    "                        break\n",
    "        \n",
    "        candidates.sort(key=lambda x: x[3], reverse=True)\n",
    "        candidates = candidates[:self.config.k_candidates]\n",
    "        \n",
    "        move_candidates: List[MoveCandidate] = []\n",
    "        for limb, hold_id, hold_pos, _, hold_type in candidates:\n",
    "            # Simulate efficiency (simplified)\n",
    "            support_points: List[Point] = []\n",
    "            for other_limb in (\"left_hand\", \"right_hand\", \"left_foot\", \"right_foot\"):\n",
    "                if other_limb == limb:\n",
    "                    support_points.append(hold_pos)\n",
    "                else:\n",
    "                    if current_row.get(f\"{other_limb}_contact_on\"):\n",
    "                        tx = _safe_float(current_row.get(f\"{other_limb}_target_x\"))\n",
    "                        ty = _safe_float(current_row.get(f\"{other_limb}_target_y\"))\n",
    "                        if tx is not None and ty is not None:\n",
    "                            support_points.append((tx, ty))\n",
    "            \n",
    "            violations: List[str] = []\n",
    "            if len(support_points) < self.config.min_support_count:\n",
    "                violations.append(f\"low_support_count_{len(support_points)}\")\n",
    "            \n",
    "            stability = _compute_stability_score(com, support_points, body_scale, self.config.stability_alpha)\n",
    "            sim_eff = stability * 0.5 + (1.0 - len(violations) * 0.1)\n",
    "            delta_eff = sim_eff - 0.5\n",
    "            \n",
    "            reasoning_parts = []\n",
    "            if delta_eff > 0:\n",
    "                reasoning_parts.append(f\"Improves efficiency by {delta_eff:.2f}\")\n",
    "            if hold_pos[1] < com_y:\n",
    "                reasoning_parts.append(\"Upward progression\")\n",
    "            if hold_type:\n",
    "                reasoning_parts.append(f\"Hold type: {hold_type}\")\n",
    "            if not violations:\n",
    "                reasoning_parts.append(\"No constraint violations\")\n",
    "            \n",
    "            reasoning = \"; \".join(reasoning_parts) if reasoning_parts else \"Candidate move\"\n",
    "            \n",
    "            move_candidates.append(\n",
    "                MoveCandidate(\n",
    "                    limb=limb,\n",
    "                    hold_id=hold_id,\n",
    "                    hold_position=hold_pos,\n",
    "                    simulated_efficiency=sim_eff,\n",
    "                    efficiency_delta=delta_eff,\n",
    "                    reasoning=reasoning,\n",
    "                    constraint_violations=violations,\n",
    "                    hold_type=hold_type,\n",
    "                )\n",
    "            )\n",
    "        \n",
    "        valid_candidates = [c for c in move_candidates if not any(\"exceeded\" in v or \"low_support\" in v for v in c.constraint_violations)]\n",
    "        if not valid_candidates:\n",
    "            valid_candidates = move_candidates\n",
    "        \n",
    "        valid_candidates.sort(key=lambda c: c.simulated_efficiency, reverse=True)\n",
    "        return valid_candidates[:top_k]\n",
    "\n",
    "\n",
    "print(\"Advanced planner module loaded\")\n",
    "print(\"Usage: planner = NextMovePlanner(); candidates = planner.plan_next_move(current_row, holds, top_k=3)\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## scripts/train_xgboost.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Train an XGBoost model on pose feature data (CLI).\"\"\"\n",
    "\n",
    "from __future__ import annotations\n",
    "\n",
    "import argparse\n",
    "from pathlib import Path\n",
    "\n",
    "from pose_ai.ml.xgb_trainer import TrainParams, params_from_dict, train_from_file\n",
    "\n",
    "\n",
    "def _parse_args() -> argparse.Namespace:\n",
    "    parser = argparse.ArgumentParser(description=\"Train XGBoost on pose feature rows.\")\n",
    "    parser.add_argument(\"features\", type=Path, help=\"Path to pose_features.json\")\n",
    "\n",
    "    # Data/label\n",
    "    parser.add_argument(\"--label-column\", default=\"detection_score\")\n",
    "    parser.add_argument(\"--label-threshold\", type=float, default=None)\n",
    "    parser.add_argument(\"--drop-columns\", nargs=\"*\", default=[\"image_path\"])\n",
    "    parser.add_argument(\"--task\", choices=[\"classification\", \"regression\"], default=\"classification\")\n",
    "    parser.add_argument(\"--test-size\", type=float, default=0.2)\n",
    "    parser.add_argument(\"--random-state\", type=int, default=42)\n",
    "\n",
    "    # XGBoost hyperparameters\n",
    "    parser.add_argument(\"--n-estimators\", type=int, default=300)\n",
    "    parser.add_argument(\"--learning-rate\", type=float, default=0.05)\n",
    "    parser.add_argument(\"--max-depth\", type=int, default=4)\n",
    "    parser.add_argument(\"--subsample\", type=float, default=0.8)\n",
    "    parser.add_argument(\"--colsample-bytree\", type=float, default=0.8)\n",
    "    parser.add_argument(\"--scale-pos-weight\", type=float, default=None)\n",
    "    parser.add_argument(\"--n-jobs\", type=int, default=0)\n",
    "    parser.add_argument(\"--tree-method\", default=None, help=\"e.g., hist or gpu_hist\")\n",
    "\n",
    "    # Training behaviour\n",
    "    parser.add_argument(\"--early-stopping-rounds\", type=int, default=30)\n",
    "    parser.add_argument(\"--eval-metric-cls\", default=\"logloss\")\n",
    "    parser.add_argument(\"--eval-metric-reg\", default=\"rmse\")\n",
    "\n",
    "    # Outputs\n",
    "    parser.add_argument(\"--model-out\", type=Path, default=Path(\"models/xgb_pose.json\"))\n",
    "    parser.add_argument(\"--metrics-out\", type=Path, default=None)\n",
    "    parser.add_argument(\"--feature-out\", type=Path, default=None)\n",
    "    parser.add_argument(\"--importance-out\", type=Path, default=None)\n",
    "    return parser.parse_args()\n",
    "\n",
    "\n",
    "def main() -> None:\n",
    "    args = _parse_args()\n",
    "    params = params_from_dict(\n",
    "        {\n",
    "            \"task\": args.task,\n",
    "            \"label_column\": args.label_column,\n",
    "            \"label_threshold\": args.label_threshold,\n",
    "            \"drop_columns\": args.drop_columns,\n",
    "            \"test_size\": args.test_size,\n",
    "            \"random_state\": args.random_state,\n",
    "            \"n_estimators\": args.n_estimators,\n",
    "            \"learning_rate\": args.learning_rate,\n",
    "            \"max_depth\": args.max_depth,\n",
    "            \"subsample\": args.subsample,\n",
    "            \"colsample_bytree\": args.colsample_bytree,\n",
    "            \"scale_pos_weight\": args.scale_pos_weight,\n",
    "            \"n_jobs\": args.n_jobs,\n",
    "            \"tree_method\": args.tree_method,\n",
    "            \"early_stopping_rounds\": args.early_stopping_rounds,\n",
    "            \"eval_metric_cls\": args.eval_metric_cls,\n",
    "            \"eval_metric_reg\": args.eval_metric_reg,\n",
    "            \"model_out\": args.model_out,\n",
    "            \"metrics_out\": args.metrics_out,\n",
    "            \"feature_out\": args.feature_out,\n",
    "            \"importance_out\": args.importance_out,\n",
    "        }\n",
    "    )\n",
    "\n",
    "    metrics = train_from_file(args.features, params)\n",
    "    print(f\"Model saved to {params.model_out}\")\n",
    "    print(\"Metrics:\", metrics)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9aa4b3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# IMU-BASED WALL ANGLE ESTIMATION (Additional Functions)\n",
    "# ============================================================================\n",
    "# These functions extend the wall angle estimation module with IMU sensor support\n",
    "\n",
    "def quaternion_to_euler(w: float, x: float, y: float, z: float) -> tuple[float, float, float]:\n",
    "    \"\"\"Convert quaternion to Euler angles (pitch, roll, yaw) in degrees.\"\"\"\n",
    "    sinr_cosp = 2.0 * (w * x + y * z)\n",
    "    cosr_cosp = 1.0 - 2.0 * (x * x + y * y)\n",
    "    roll = np.arctan2(sinr_cosp, cosr_cosp)\n",
    "    \n",
    "    sinp = 2.0 * (w * y - z * x)\n",
    "    if abs(sinp) >= 1:\n",
    "        pitch = np.copysign(np.pi / 2, sinp)\n",
    "    else:\n",
    "        pitch = np.arcsin(sinp)\n",
    "    \n",
    "    siny_cosp = 2.0 * (w * z + x * y)\n",
    "    cosy_cosp = 1.0 - 2.0 * (y * y + z * z)\n",
    "    yaw = np.arctan2(siny_cosp, cosy_cosp)\n",
    "    \n",
    "    return np.degrees(pitch), np.degrees(roll), np.degrees(yaw)\n",
    "\n",
    "\n",
    "def compute_wall_angle_from_imu(\n",
    "    quaternion: list[float] | None = None,\n",
    "    euler_angles: list[float] | None = None,\n",
    ") -> WallAngleResult:\n",
    "    \"\"\"Compute wall angle from IMU sensor data.\n",
    "    \n",
    "    Args:\n",
    "        quaternion: Device orientation as [w, x, y, z]\n",
    "        euler_angles: Device orientation as [pitch, roll, yaw] in degrees\n",
    "    \n",
    "    Returns:\n",
    "        WallAngleResult with angle in degrees (0=horizontal, 90=vertical)\n",
    "    \"\"\"\n",
    "    if quaternion is not None:\n",
    "        if len(quaternion) != 4:\n",
    "            return WallAngleResult(angle_degrees=None, confidence=0.0, method=\"imu_error\", hough_lines=[])\n",
    "        w, x, y, z = quaternion\n",
    "        pitch, roll, yaw = quaternion_to_euler(w, x, y, z)\n",
    "    elif euler_angles is not None:\n",
    "        if len(euler_angles) != 3:\n",
    "            return WallAngleResult(angle_degrees=None, confidence=0.0, method=\"imu_error\", hough_lines=[])\n",
    "        pitch, roll, yaw = euler_angles\n",
    "    else:\n",
    "        return WallAngleResult(angle_degrees=None, confidence=0.0, method=\"imu_missing\", hough_lines=[])\n",
    "    \n",
    "    wall_angle = abs(pitch)\n",
    "    wall_angle = max(0.0, min(180.0, wall_angle))\n",
    "    confidence = 1.0\n",
    "    if roll > 45 or roll < -45:\n",
    "        confidence = 0.7\n",
    "    \n",
    "    return WallAngleResult(\n",
    "        angle_degrees=wall_angle,\n",
    "        confidence=confidence,\n",
    "        method=\"imu_sensor\",\n",
    "        hough_lines=[],\n",
    "        pca_angle=None,\n",
    "    )\n",
    "\n",
    "\n",
    "print(\"IMU wall angle functions loaded\")\n",
    "print(\"Usage: compute_wall_angle_from_imu(quaternion=[w,x,y,z] or euler_angles=[pitch,roll,yaw])\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e27cda12",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## scripts/visualize_pose.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Generate pose visualization overlays from pose_results.json.\"\"\"\n",
    "\n",
    "from __future__ import annotations\n",
    "\n",
    "import argparse\n",
    "import json\n",
    "from pathlib import Path\n",
    "\n",
    "import cv2\n",
    "\n",
    "try:  # optional dependency for connection definitions\n",
    "    from mediapipe.python.solutions.pose import PoseLandmark, POSE_CONNECTIONS\n",
    "except ModuleNotFoundError:  # fallback if mediapipe not installed\n",
    "    PoseLandmark = None\n",
    "    POSE_CONNECTIONS = []\n",
    "\n",
    "\n",
    "DEFAULT_CONNECTIONS = [\n",
    "    (\"left_shoulder\", \"right_shoulder\"),\n",
    "    (\"left_shoulder\", \"left_elbow\"),\n",
    "    (\"left_elbow\", \"left_wrist\"),\n",
    "    (\"right_shoulder\", \"right_elbow\"),\n",
    "    (\"right_elbow\", \"right_wrist\"),\n",
    "    (\"left_hip\", \"right_hip\"),\n",
    "    (\"left_shoulder\", \"left_hip\"),\n",
    "    (\"right_shoulder\", \"right_hip\"),\n",
    "    (\"left_hip\", \"left_knee\"),\n",
    "    (\"left_knee\", \"left_ankle\"),\n",
    "    (\"right_hip\", \"right_knee\"),\n",
    "    (\"right_knee\", \"right_ankle\"),\n",
    "]\n",
    "\n",
    "\n",
    "def build_parser() -> argparse.ArgumentParser:\n",
    "    parser = argparse.ArgumentParser(description=\"Visualize pose landmarks on extracted frames.\")\n",
    "    parser.add_argument(\"pose_results\", type=Path, help=\"Path to pose_results.json\")\n",
    "    parser.add_argument(\n",
    "        \"--output\", type=Path, default=None,\n",
    "        help=\"Directory to write visualized images (defaults to frame directory).\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--include-missing\", action=\"store_true\",\n",
    "        help=\"Include frames even when detection score/visibility is low.\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--min-score\",\n",
    "        type=float,\n",
    "        default=0.3,\n",
    "        help=\"Minimum frame detection score required for visualization (default: 0.3).\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--min-visibility\",\n",
    "        type=float,\n",
    "        default=0.2,\n",
    "        help=\"Minimum landmark visibility to draw a point/connection (default: 0.2).\",\n",
    "    )\n",
    "    return parser\n",
    "\n",
    "\n",
    "def _get_connections():\n",
    "    if PoseLandmark is None or not POSE_CONNECTIONS:\n",
    "        return DEFAULT_CONNECTIONS\n",
    "    connections = []\n",
    "    for a_idx, b_idx in POSE_CONNECTIONS:\n",
    "        connections.append((PoseLandmark(a_idx).name.lower(), PoseLandmark(b_idx).name.lower()))\n",
    "    return connections\n",
    "\n",
    "\n",
    "def visualize_pose_results(\n",
    "    pose_results_path: Path,\n",
    "    output_dir: Path | None = None,\n",
    "    *,\n",
    "    include_missing: bool = False,\n",
    "    min_score: float = 0.3,\n",
    "    min_visibility: float = 0.2,\n",
    ") -> int:\n",
    "    payload = json.loads(pose_results_path.read_text(encoding=\"utf-8\"))\n",
    "    frames = payload.get(\"frames\", [])\n",
    "    count = 0\n",
    "    connections = _get_connections()\n",
    "\n",
    "    for frame in frames:\n",
    "        image_path = Path(frame[\"image_path\"])\n",
    "        image = cv2.imread(str(image_path))\n",
    "        if image is None:\n",
    "            continue\n",
    "        height, width = image.shape[:2]\n",
    "        landmarks = frame.get(\"landmarks\", [])\n",
    "        detection_score = float(frame.get(\"detection_score\", 0.0))\n",
    "        if not include_missing and detection_score < min_score:\n",
    "            continue\n",
    "        if not landmarks and not include_missing:\n",
    "            continue\n",
    "\n",
    "        points = {}\n",
    "        for landmark in landmarks:\n",
    "            x = int(landmark[\"x\"] * width)\n",
    "            y = int(landmark[\"y\"] * height)\n",
    "            if landmark.get(\"visibility\", 1.0) < min_visibility:\n",
    "                continue\n",
    "            points[landmark[\"name\"]] = (x, y)\n",
    "            cv2.circle(image, (x, y), 4, (0, 255, 0), -1)\n",
    "\n",
    "        for start, end in connections:\n",
    "            if start in points and end in points:\n",
    "                cv2.line(image, points[start], points[end], (255, 0, 0), 2)\n",
    "\n",
    "        target_dir = output_dir or image_path.parent / \"visualized\"\n",
    "        target_dir.mkdir(parents=True, exist_ok=True)\n",
    "        out_path = target_dir / f\"{image_path.stem}_viz{image_path.suffix}\"\n",
    "        cv2.imwrite(str(out_path), image)\n",
    "        count += 1\n",
    "    return count\n",
    "\n",
    "\n",
    "def main() -> None:\n",
    "    parser = build_parser()\n",
    "    args = parser.parse_args()\n",
    "    processed = visualize_pose_results(\n",
    "        args.pose_results,\n",
    "        args.output,\n",
    "        include_missing=args.include_missing,\n",
    "        min_score=args.min_score,\n",
    "        min_visibility=args.min_visibility,\n",
    "    )\n",
    "    print(f\"Saved {processed} annotated frames\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82eab9b7",
   "metadata": {},
   "source": [
    "## Wall Angle Estimation Module\n",
    "\n",
    "Automatic wall angle estimation using Hough line detection and PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45dd4caf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# WALL ANGLE ESTIMATION\n",
    "# ============================================================================\n",
    "# Purpose: Automatically estimate climbing wall angle from video frames\n",
    "# Method: Combines Hough line detection with optional PCA on hold positions\n",
    "# Output: Angle in degrees (0 = horizontal, 90 = vertical wall)\n",
    "# ============================================================================\n",
    "\n",
    "from __future__ import annotations\n",
    "from dataclasses import dataclass\n",
    "from pathlib import Path\n",
    "from typing import List, Optional, Sequence, Tuple\n",
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "# Data structure for wall angle estimation results\n",
    "@dataclass(slots=True)\n",
    "class WallAngleResult:\n",
    "    angle_degrees: float | None\n",
    "    confidence: float\n",
    "    method: str\n",
    "    hough_lines: List[Tuple[Tuple[int, int], Tuple[int, int]]]\n",
    "    pca_angle: float | None = None\n",
    "\n",
    "    def as_dict(self) -> dict[str, object]:\n",
    "        return {\n",
    "            \"angle_degrees\": self.angle_degrees,\n",
    "            \"confidence\": self.confidence,\n",
    "            \"method\": self.method,\n",
    "            \"pca_angle\": self.pca_angle,\n",
    "            \"hough_line_count\": len(self.hough_lines),\n",
    "        }\n",
    "\n",
    "\n",
    "def estimate_wall_angle(\n",
    "    image_path: Path | str,\n",
    "    *,\n",
    "    hold_centers: Optional[list] = None,\n",
    "    canny_threshold1: int = 50,\n",
    "    canny_threshold2: int = 150,\n",
    "    hough_threshold: int = 120,\n",
    ") -> WallAngleResult:\n",
    "    \"\"\"\n",
    "    Estimate wall angle from a single frame image.\n",
    "    \n",
    "    Algorithm:\n",
    "    1. Convert image to grayscale\n",
    "    2. Apply Canny edge detection\n",
    "    3. Use Hough line transform to find dominant lines\n",
    "    4. Calculate mean angle from detected lines\n",
    "    5. Optionally refine with PCA on hold center coordinates\n",
    "    6. Blend estimates if both methods agree\n",
    "    \n",
    "    Parameters:\n",
    "        image_path: Path to frame image\n",
    "        hold_centers: Optional list of (x, y) hold coordinates for PCA refinement\n",
    "        canny_threshold1: Lower threshold for Canny edge detection\n",
    "        canny_threshold2: Upper threshold for Canny edge detection\n",
    "        hough_threshold: Minimum votes for Hough line detection\n",
    "        \n",
    "    Returns:\n",
    "        WallAngleResult with angle estimate and confidence score\n",
    "    \"\"\"\n",
    "    path = Path(image_path)\n",
    "    image = cv2.imread(str(path))\n",
    "    if image is None:\n",
    "        return WallAngleResult(\n",
    "            angle_degrees=None, \n",
    "            confidence=0.0, \n",
    "            method=\"load-error\", \n",
    "            hough_lines=[]\n",
    "        )\n",
    "    \n",
    "    # Step 1: Prepare image for edge detection\n",
    "    gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "    edges = cv2.Canny(gray, canny_threshold1, canny_threshold2, L2gradient=True)\n",
    "    \n",
    "    # Step 2: Detect lines using probabilistic Hough transform\n",
    "    lines_p = cv2.HoughLinesP(\n",
    "        edges, \n",
    "        rho=1, \n",
    "        theta=np.pi/180.0, \n",
    "        threshold=hough_threshold,\n",
    "        minLineLength=60, \n",
    "        maxLineGap=10\n",
    "    )\n",
    "    \n",
    "    # Step 3: Extract line endpoints\n",
    "    normalized_lines = []\n",
    "    if lines_p is not None:\n",
    "        for line in lines_p:\n",
    "            x1, y1, x2, y2 = line[0]\n",
    "            normalized_lines.append(((int(x1), int(y1)), (int(x2), int(y2))))\n",
    "    \n",
    "    # Step 4: Compute angle from Hough lines\n",
    "    angles = []\n",
    "    for (x1, y1), (x2, y2) in normalized_lines:\n",
    "        dx, dy = x2 - x1, y2 - y1\n",
    "        if abs(dx) < 1e-6 and abs(dy) < 1e-6:\n",
    "            continue\n",
    "        angle = float(np.degrees(np.arctan2(dy, dx)))\n",
    "        if angle < 0:\n",
    "            angle += 180.0\n",
    "        angles.append(angle)\n",
    "    \n",
    "    hough_angle = None\n",
    "    if angles:\n",
    "        # Use circular mean for angle averaging\n",
    "        radians = np.radians(angles)\n",
    "        mean = float(np.degrees(np.arctan2(np.sum(np.sin(radians)), np.sum(np.cos(radians)))))\n",
    "        if mean < 0:\n",
    "            mean += 180.0\n",
    "        hough_angle = mean\n",
    "    \n",
    "    # Step 5: Optional PCA refinement using hold positions\n",
    "    pca_angle_value = None\n",
    "    if hold_centers:\n",
    "        pts = np.array(list(hold_centers), dtype=float)\n",
    "        # Scale normalized coordinates to pixel space\n",
    "        if pts.max() <= 1.2:\n",
    "            h, w = gray.shape[:2]\n",
    "            pts[:, 0] *= w\n",
    "            pts[:, 1] *= h\n",
    "        if pts.shape[0] >= 3:\n",
    "            centered = pts - np.mean(pts, axis=0, keepdims=True)\n",
    "            cov = np.cov(centered.T)\n",
    "            eigvals, eigvecs = np.linalg.eig(cov)\n",
    "            idx = int(np.argmax(eigvals))\n",
    "            principal = eigvecs[:, idx]\n",
    "            pca_angle_value = float(np.degrees(np.arctan2(principal[1], principal[0])))\n",
    "            if pca_angle_value < 0:\n",
    "                pca_angle_value += 180.0\n",
    "    \n",
    "    # Step 6: Combine estimates with confidence weighting\n",
    "    if hough_angle is not None and pca_angle_value is not None:\n",
    "        diff = abs(hough_angle - pca_angle_value)\n",
    "        if diff < 15.0:  # Agreement threshold\n",
    "            final_angle = (hough_angle + pca_angle_value) / 2.0\n",
    "            method, confidence = \"hough+pca\", 0.9\n",
    "        else:  # Disagreement - prefer Hough but lower confidence\n",
    "            final_angle, method, confidence = hough_angle, \"hough\", 0.6\n",
    "    elif hough_angle is not None:\n",
    "        final_angle = hough_angle\n",
    "        confidence = 0.7 if len(normalized_lines) >= 5 else 0.5\n",
    "        method = \"hough\"\n",
    "    else:\n",
    "        final_angle = pca_angle_value\n",
    "        confidence = 0.4 if pca_angle_value is not None else 0.0\n",
    "        method = \"pca\" if pca_angle_value is not None else \"none\"\n",
    "    \n",
    "    return WallAngleResult(\n",
    "        angle_degrees=final_angle,\n",
    "        confidence=confidence,\n",
    "        method=method,\n",
    "        hough_lines=normalized_lines,\n",
    "        pca_angle=pca_angle_value,\n",
    "    )\n",
    "\n",
    "\n",
    "# Module loaded confirmation\n",
    "print(\"Wall angle estimation module loaded\")\n",
    "print(\"Usage: estimate_wall_angle(image_path, hold_centers=[(x1,y1), ...])\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40238da1",
   "metadata": {},
   "source": [
    "## Hold Detection and Clustering Module\n",
    "\n",
    "YOLO-based hold detection with DBSCAN clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23a8769b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# HOLD DETECTION AND CLUSTERING\n",
    "# ============================================================================\n",
    "# Purpose: Detect climbing holds from frames and cluster into stable positions\n",
    "# Method: YOLO object detection + DBSCAN spatial clustering\n",
    "# Output: List of unique holds with normalized coordinates\n",
    "# ============================================================================\n",
    "\n",
    "from __future__ import annotations\n",
    "from dataclasses import dataclass\n",
    "from pathlib import Path\n",
    "from typing import List, Sequence\n",
    "import json\n",
    "import numpy as np\n",
    "\n",
    "try:\n",
    "    from ultralytics import YOLO as UltralyticsYOLO\n",
    "    YOLO_AVAILABLE = True\n",
    "except ModuleNotFoundError:\n",
    "    UltralyticsYOLO = None\n",
    "    YOLO_AVAILABLE = False\n",
    "    print(\"Warning: ultralytics not installed\")\n",
    "    print(\"Install with: pip install ultralytics\")\n",
    "\n",
    "\n",
    "# Data structures for hold detection\n",
    "@dataclass(slots=True)\n",
    "class HoldDetection:\n",
    "    \"\"\"Single hold detection from one frame\"\"\"\n",
    "    frame_index: int\n",
    "    label: str\n",
    "    confidence: float\n",
    "    x_center: float  # normalized 0-1\n",
    "    y_center: float\n",
    "    width: float\n",
    "    height: float\n",
    "\n",
    "\n",
    "@dataclass(slots=True)\n",
    "class ClusteredHold:\n",
    "    \"\"\"Aggregated hold position from multiple detections\"\"\"\n",
    "    hold_id: str\n",
    "    label: str\n",
    "    x: float\n",
    "    y: float\n",
    "    radius: float\n",
    "    detections: int\n",
    "    avg_confidence: float\n",
    "\n",
    "    def as_dict(self) -> dict[str, object]:\n",
    "        return {\n",
    "            \"hold_id\": self.hold_id,\n",
    "            \"label\": self.label,\n",
    "            \"coords\": [self.x, self.y],\n",
    "            \"radius\": self.radius,\n",
    "            \"detections\": self.detections,\n",
    "            \"avg_confidence\": self.avg_confidence,\n",
    "            \"normalized\": True,\n",
    "        }\n",
    "\n",
    "\n",
    "def detect_holds(\n",
    "    image_paths: Sequence[Path],\n",
    "    *,\n",
    "    model_name: str = \"yolov8n.pt\",\n",
    "    device: str | None = None,\n",
    "    hold_labels: tuple = (\"hold\", \"foot_hold\", \"volume\", \"jug\", \"crimp\", \"sloper\", \"pinch\"),\n",
    ") -> List[HoldDetection]:\n",
    "    \"\"\"\n",
    "    Run YOLO object detection to find holds in frame images.\n",
    "    \n",
    "    Process:\n",
    "    1. Load YOLO model (pre-trained or fine-tuned)\n",
    "    2. Run inference on all frames in batch\n",
    "    3. Filter detections to hold-related classes only\n",
    "    4. Normalize bounding box coordinates to [0,1] range\n",
    "    \n",
    "    Parameters:\n",
    "        image_paths: List of frame image paths\n",
    "        model_name: YOLO model weights file (e.g., 'yolov8n.pt' or custom)\n",
    "        device: Torch device ('cpu', 'cuda:0', etc.)\n",
    "        hold_labels: Tuple of class labels to keep\n",
    "        \n",
    "    Returns:\n",
    "        List of HoldDetection objects with normalized coordinates\n",
    "    \"\"\"\n",
    "    if not YOLO_AVAILABLE:\n",
    "        print(\"YOLO not available - skipping hold detection\")\n",
    "        return []\n",
    "    \n",
    "    model = UltralyticsYOLO(model_name)\n",
    "    if not image_paths:\n",
    "        return []\n",
    "    \n",
    "    # Run batch inference\n",
    "    results = model.predict(\n",
    "        source=[str(p) for p in image_paths], \n",
    "        device=device, \n",
    "        imgsz=640, \n",
    "        stream=False, \n",
    "        verbose=False\n",
    "    )\n",
    "    \n",
    "    detections = []\n",
    "    \n",
    "    # Process each frame result\n",
    "    for frame_idx, result in enumerate(results):\n",
    "        boxes = getattr(result, \"boxes\", None)\n",
    "        if boxes is None:\n",
    "            continue\n",
    "        \n",
    "        # Extract detection data (move from GPU if needed)\n",
    "        xyxy = boxes.xyxy.cpu().tolist() if hasattr(boxes.xyxy, \"cpu\") else boxes.xyxy.tolist()\n",
    "        cls = boxes.cls.cpu().tolist() if hasattr(boxes.cls, \"cpu\") else boxes.cls.tolist()\n",
    "        conf = boxes.conf.cpu().tolist() if hasattr(boxes.conf, \"cpu\") else boxes.conf.tolist()\n",
    "        names = result.names or {}\n",
    "        \n",
    "        # Filter and normalize hold detections\n",
    "        for box_idx, bbox in enumerate(xyxy):\n",
    "            class_idx = int(cls[box_idx]) if box_idx < len(cls) else -1\n",
    "            label = str(names.get(class_idx, class_idx)).lower()\n",
    "            confidence = float(conf[box_idx]) if box_idx < len(conf) else 0.0\n",
    "            \n",
    "            # Keep only hold-related classes\n",
    "            if label not in hold_labels:\n",
    "                continue\n",
    "            \n",
    "            # Convert bbox to normalized center + size\n",
    "            x1, y1, x2, y2 = (float(v) for v in bbox[:4])\n",
    "            width = max(1e-6, x2 - x1)\n",
    "            height = max(1e-6, y2 - y1)\n",
    "            h, w = result.orig_shape\n",
    "            \n",
    "            detections.append(HoldDetection(\n",
    "                frame_index=frame_idx,\n",
    "                label=label,\n",
    "                confidence=confidence,\n",
    "                x_center=(x1 + width/2.0) / w,\n",
    "                y_center=(y1 + height/2.0) / h,\n",
    "                width=width / w,\n",
    "                height=height / h,\n",
    "            ))\n",
    "    \n",
    "    return detections\n",
    "\n",
    "\n",
    "def cluster_holds(\n",
    "    detections: Sequence[HoldDetection], \n",
    "    *, \n",
    "    eps: float = 0.03, \n",
    "    min_samples: int = 3\n",
    ") -> List[ClusteredHold]:\n",
    "    \"\"\"\n",
    "    Cluster hold detections across frames to find stable hold positions.\n",
    "    \n",
    "    Algorithm:\n",
    "    1. Extract 2D coordinates from all detections\n",
    "    2. Apply DBSCAN density-based clustering\n",
    "    3. For each cluster, compute centroid and statistics\n",
    "    4. Assign unique hold IDs to each cluster\n",
    "    \n",
    "    Parameters:\n",
    "        detections: List of hold detections from all frames\n",
    "        eps: DBSCAN epsilon (max distance between points in cluster)\n",
    "        min_samples: Minimum detections required to form cluster\n",
    "        \n",
    "    Returns:\n",
    "        List of ClusteredHold objects representing unique holds\n",
    "    \"\"\"\n",
    "    if not detections:\n",
    "        return []\n",
    "    \n",
    "    points = np.array([[d.x_center, d.y_center] for d in detections], dtype=float)\n",
    "    labels_arr = np.array([d.label for d in detections], dtype=object)\n",
    "    confidences = np.array([d.confidence for d in detections], dtype=float)\n",
    "    \n",
    "    # Apply DBSCAN clustering\n",
    "    try:\n",
    "        from sklearn.cluster import DBSCAN\n",
    "        clustering = DBSCAN(eps=eps, min_samples=min_samples).fit(points)\n",
    "        cluster_ids = clustering.labels_\n",
    "    except Exception:\n",
    "        # Fallback: no clustering\n",
    "        cluster_ids = np.full(points.shape[0], -1, dtype=int)\n",
    "    \n",
    "    unique_ids = [cid for cid in sorted(set(int(c) for c in cluster_ids)) if cid >= 0]\n",
    "    clustered = []\n",
    "    \n",
    "    if not unique_ids:\n",
    "        # Fallback: treat each detection as unique hold\n",
    "        for idx, d in enumerate(detections):\n",
    "            clustered.append(ClusteredHold(\n",
    "                hold_id=f\"hold_{idx}\",\n",
    "                label=d.label,\n",
    "                x=d.x_center,\n",
    "                y=d.y_center,\n",
    "                radius=max(d.width, d.height) / 2.0,\n",
    "                detections=1,\n",
    "                avg_confidence=d.confidence,\n",
    "            ))\n",
    "        return clustered\n",
    "    \n",
    "    # Aggregate clusters\n",
    "    for cid in unique_ids:\n",
    "        mask = cluster_ids == cid\n",
    "        cluster_pts = points[mask]\n",
    "        cluster_labels = labels_arr[mask]\n",
    "        cluster_conf = confidences[mask]\n",
    "        \n",
    "        # Compute cluster centroid\n",
    "        x_mean = float(cluster_pts[:, 0].mean())\n",
    "        y_mean = float(cluster_pts[:, 1].mean())\n",
    "        \n",
    "        # Compute cluster radius (mean distance to centroid)\n",
    "        dists = np.linalg.norm(cluster_pts - np.array([[x_mean, y_mean]]), axis=1)\n",
    "        radius = float(dists.mean() + 0.01)\n",
    "        \n",
    "        # Find dominant label\n",
    "        lbl_values, counts = np.unique(cluster_labels, return_counts=True)\n",
    "        dominant_label = str(lbl_values[int(np.argmax(counts))])\n",
    "        \n",
    "        clustered.append(ClusteredHold(\n",
    "            hold_id=f\"hold_{cid}\",\n",
    "            label=dominant_label,\n",
    "            x=x_mean,\n",
    "            y=y_mean,\n",
    "            radius=radius,\n",
    "            detections=int(mask.sum()),\n",
    "            avg_confidence=float(cluster_conf.mean()),\n",
    "        ))\n",
    "    \n",
    "    return clustered\n",
    "\n",
    "\n",
    "def export_holds_json(\n",
    "    clustered: Sequence[ClusteredHold], \n",
    "    *, \n",
    "    output_path: Path | str\n",
    ") -> Path:\n",
    "    \"\"\"\n",
    "    Export clustered holds to JSON file.\n",
    "    \n",
    "    Output format:\n",
    "    {\n",
    "        \"hold_0\": {\"coords\": [x, y], \"label\": \"jug\", ...},\n",
    "        \"hold_1\": {\"coords\": [x, y], \"label\": \"crimp\", ...},\n",
    "        ...\n",
    "    }\n",
    "    \"\"\"\n",
    "    path = Path(output_path)\n",
    "    path.parent.mkdir(parents=True, exist_ok=True)\n",
    "    payload = {hold.hold_id: hold.as_dict() for hold in clustered}\n",
    "    path.write_text(json.dumps(payload, indent=2), encoding=\"utf-8\")\n",
    "    return path\n",
    "\n",
    "\n",
    "# Module loaded confirmation\n",
    "print(\"Hold detection and clustering module loaded\")\n",
    "print(f\"YOLO available: {YOLO_AVAILABLE}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5394151",
   "metadata": {},
   "source": [
    "## Efficiency Scoring and Recommendation Module\n",
    "\n",
    "Heuristic-based efficiency scoring and next hold recommendation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2ad8595",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# EFFICIENCY SCORING AND RECOMMENDATION\n",
    "# ============================================================================\n",
    "# Purpose: Calculate movement efficiency and suggest next holds\n",
    "# Method: Multi-component heuristic scoring + proximity-based ranking\n",
    "# Output: Efficiency score (0-1) and ranked hold suggestions\n",
    "# ============================================================================\n",
    "\n",
    "from __future__ import annotations\n",
    "from dataclasses import dataclass\n",
    "from typing import List, Sequence\n",
    "import math\n",
    "\n",
    "# Weight configuration for efficiency components\n",
    "WEIGHTS = {\n",
    "    \"detection\": 0.30,  # Pose detection quality\n",
    "    \"joint\": 0.20,      # Joint angle efficiency (less extreme = better)\n",
    "    \"com\": 0.20,        # Center of mass stability\n",
    "    \"contact\": 0.20,    # Contact point stability\n",
    "    \"hip\": 0.10,        # Hip alignment with wall\n",
    "}\n",
    "\n",
    "\n",
    "@dataclass(slots=True)\n",
    "class EfficiencyResult:\n",
    "    \"\"\"Efficiency score with component breakdown\"\"\"\n",
    "    score: float\n",
    "    components: dict[str, float]\n",
    "\n",
    "    def as_dict(self) -> dict[str, float]:\n",
    "        payload = {\"score\": self.score}\n",
    "        payload.update(self.components)\n",
    "        return payload\n",
    "\n",
    "\n",
    "def _safe_float(value) -> float:\n",
    "    \"\"\"Convert value to float, return NaN on failure\"\"\"\n",
    "    try:\n",
    "        return float(value)\n",
    "    except Exception:\n",
    "        return float(\"nan\")\n",
    "\n",
    "\n",
    "def efficiency_from_frames(\n",
    "    frame_rows: Sequence[dict[str, object]], \n",
    "    window: int = 5\n",
    ") -> EfficiencyResult:\n",
    "    \"\"\"\n",
    "    Calculate efficiency score from pose feature frames.\n",
    "    \n",
    "    Algorithm:\n",
    "    1. Take last N frames (window) for recent movement context\n",
    "    2. Compute 5 components:\n",
    "       - detection: Average pose detection quality\n",
    "       - joint: Joint angle efficiency (normalized by 180 degrees)\n",
    "       - com: Center of mass stability (inverse variance)\n",
    "       - contact: Limb contact stability (hold consistency)\n",
    "       - hip: Hip alignment with wall angle\n",
    "    3. Weighted average of components\n",
    "    \n",
    "    Parameters:\n",
    "        frame_rows: List of feature dictionaries (from pose_features.json)\n",
    "        window: Number of recent frames to analyze\n",
    "        \n",
    "    Returns:\n",
    "        EfficiencyResult with overall score and component breakdown\n",
    "    \"\"\"\n",
    "    if not frame_rows:\n",
    "        return EfficiencyResult(score=float(\"nan\"), components={})\n",
    "    \n",
    "    # Use recent frames for temporal context\n",
    "    recent = list(frame_rows[-window:]) if len(frame_rows) >= window else list(frame_rows)\n",
    "    \n",
    "    # Component 1: Detection quality\n",
    "    detection_scores = [\n",
    "        _safe_float(row.get(\"detection_score\")) \n",
    "        for row in recent \n",
    "        if row.get(\"detection_score\") is not None\n",
    "    ]\n",
    "    detection_component = (\n",
    "        float(sum(detection_scores) / len(detection_scores)) \n",
    "        if detection_scores else 0.0\n",
    "    )\n",
    "    \n",
    "    # Component 2: Joint efficiency (lower angles = more efficient)\n",
    "    joint_keys = [k for k in recent[-1].keys() if k.endswith(\"_angle\")]\n",
    "    joint_values = []\n",
    "    for row in recent:\n",
    "        for key in joint_keys:\n",
    "            v = row.get(key)\n",
    "            if isinstance(v, (int, float)):\n",
    "                joint_values.append(abs(float(v)))\n",
    "    \n",
    "    joint_component = (\n",
    "        1.0 - (sum(joint_values) / (len(joint_values) * 180.0)) \n",
    "        if joint_values else 0.5\n",
    "    )\n",
    "    \n",
    "    # Component 3: COM stability (perpendicular to wall)\n",
    "    com_perp_vals = [\n",
    "        _safe_float(row.get(\"com_perp_wall\")) \n",
    "        for row in recent \n",
    "        if row.get(\"com_perp_wall\") is not None\n",
    "    ]\n",
    "    \n",
    "    if len(com_perp_vals) >= 2:\n",
    "        mean_val = sum(com_perp_vals) / len(com_perp_vals)\n",
    "        variance = sum((v - mean_val) ** 2 for v in com_perp_vals) / len(com_perp_vals)\n",
    "        max_var = max(variance, 1e-6)\n",
    "        com_component = 1.0 / (1.0 + variance / max_var)\n",
    "    else:\n",
    "        com_component = 0.5\n",
    "    \n",
    "    # Component 4: Contact stability (limbs staying on same holds)\n",
    "    contact_keys = [k for k in recent[-1].keys() if k.endswith(\"_target\")]\n",
    "    stable_counts = 0\n",
    "    total_contacts = 0\n",
    "    \n",
    "    if contact_keys and len(recent) >= 2:\n",
    "        prev_row = recent[0]\n",
    "        for row in recent[1:]:\n",
    "            for key in contact_keys:\n",
    "                if key in row and key in prev_row:\n",
    "                    total_contacts += 1\n",
    "                    if row.get(key) == prev_row.get(key):\n",
    "                        stable_counts += 1\n",
    "            prev_row = row\n",
    "    \n",
    "    contact_component = (stable_counts / total_contacts) if total_contacts else 0.5\n",
    "    \n",
    "    # Component 5: Hip alignment with wall\n",
    "    hip_alignment = _safe_float(recent[-1].get(\"hip_alignment_error\"))\n",
    "    hip_component = (\n",
    "        1.0 - (hip_alignment / 90.0) \n",
    "        if hip_alignment == hip_alignment  # Check for NaN\n",
    "        else 0.5\n",
    "    )\n",
    "    \n",
    "    # Weighted combination\n",
    "    score = (\n",
    "        WEIGHTS[\"detection\"] * detection_component\n",
    "        + WEIGHTS[\"joint\"] * joint_component\n",
    "        + WEIGHTS[\"com\"] * com_component\n",
    "        + WEIGHTS[\"contact\"] * contact_component\n",
    "        + WEIGHTS[\"hip\"] * hip_component\n",
    "    )\n",
    "    \n",
    "    return EfficiencyResult(\n",
    "        score=score,\n",
    "        components={\n",
    "            \"detection_component\": detection_component,\n",
    "            \"joint_component\": joint_component,\n",
    "            \"com_component\": com_component,\n",
    "            \"contact_component\": contact_component,\n",
    "            \"hip_component\": hip_component,\n",
    "        },\n",
    "    )\n",
    "\n",
    "\n",
    "def suggest_next_holds(\n",
    "    current_row: dict[str, object],\n",
    "    all_holds: Sequence[dict[str, object]],\n",
    "    *,\n",
    "    top_k: int = 3,\n",
    ") -> List[dict[str, object]]:\n",
    "    \"\"\"\n",
    "    Suggest next holds based on proximity and novelty.\n",
    "    \n",
    "    Ranking heuristic:\n",
    "        score = 0.7 * (1 - distance) + 0.3 * novelty\n",
    "        \n",
    "    Where:\n",
    "        - distance: Euclidean distance from COM to hold (normalized)\n",
    "        - novelty: 1 if hold not currently contacted, 0 otherwise\n",
    "    \n",
    "    Parameters:\n",
    "        current_row: Latest feature row (current climber state)\n",
    "        all_holds: List of available holds (from holds.json)\n",
    "        top_k: Number of holds to recommend\n",
    "        \n",
    "    Returns:\n",
    "        List of top-k hold dictionaries sorted by score\n",
    "    \"\"\"\n",
    "    com_x = _safe_float(current_row.get(\"com_x\"))\n",
    "    com_y = _safe_float(current_row.get(\"com_y\"))\n",
    "    \n",
    "    # Require valid COM position\n",
    "    if any(math.isnan(v) for v in (com_x, com_y)):\n",
    "        return []\n",
    "    \n",
    "    # Track currently used holds\n",
    "    used_targets = {\n",
    "        str(current_row.get(k))\n",
    "        for k in current_row.keys()\n",
    "        if k.endswith(\"_target\") and current_row.get(k)\n",
    "    }\n",
    "    \n",
    "    # Rank all holds\n",
    "    ranked = []\n",
    "    for hold in all_holds:\n",
    "        coords = hold.get(\"coords\")\n",
    "        if not isinstance(coords, (list, tuple)) or len(coords) != 2:\n",
    "            continue\n",
    "        \n",
    "        hx, hy = float(coords[0]), float(coords[1])\n",
    "        \n",
    "        # Calculate distance component\n",
    "        dist = math.sqrt((hx - com_x) ** 2 + (hy - com_y) ** 2)\n",
    "        dist_norm = min(dist, 1.0)\n",
    "        \n",
    "        # Calculate novelty component\n",
    "        novel = 0 if hold.get(\"hold_id\") in used_targets else 1\n",
    "        \n",
    "        # Combined score\n",
    "        score = 0.7 * (1.0 - dist_norm) + 0.3 * novel\n",
    "        ranked.append((score, hold))\n",
    "    \n",
    "    # Sort and return top-k\n",
    "    ranked.sort(key=lambda t: t[0], reverse=True)\n",
    "    return [hold for _, hold in ranked[:top_k]]\n",
    "\n",
    "\n",
    "# Module loaded confirmation\n",
    "print(\"Efficiency and recommendation module loaded\")\n",
    "print(\"Component weights:\", WEIGHTS)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ffacc58",
   "metadata": {},
   "source": [
    "## Complete Pipeline Demo\n",
    "\n",
    "End-to-end demonstration of all new features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59ce92aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# COMPLETE PIPELINE DEMONSTRATION\n",
    "# ============================================================================\n",
    "# Purpose: Show integration of all new features in end-to-end workflow\n",
    "# Steps:\n",
    "#   1. Hold detection and clustering\n",
    "#   2. Wall angle estimation\n",
    "#   3. Load pose features\n",
    "#   4. Calculate efficiency score\n",
    "#   5. Generate hold recommendations\n",
    "# ============================================================================\n",
    "\n",
    "from pathlib import Path\n",
    "import json\n",
    "\n",
    "# Configuration: Set your frame directory path\n",
    "FRAME_DIR = Path(\"data/frames/video01\")  # Change to your actual path\n",
    "manifest_path = FRAME_DIR / \"manifest.json\"\n",
    "pose_results_path = FRAME_DIR / \"pose_results.json\"\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"BetaMove Enhanced Pipeline Demo\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# ----------------------------------------------------------------------------\n",
    "# STEP 1: Hold Detection and Clustering\n",
    "# ----------------------------------------------------------------------------\n",
    "print(\"\\n[Step 1/5] Hold Detection and Clustering\")\n",
    "print(\"-\" * 60)\n",
    "\n",
    "if YOLO_AVAILABLE:\n",
    "    # Get sample frames (limit to 20 for demo speed)\n",
    "    image_paths = sorted(FRAME_DIR.glob(\"*.jpg\"))[:20]\n",
    "    \n",
    "    if image_paths:\n",
    "        # Run YOLO detection\n",
    "        holds_detections = detect_holds(image_paths, model_name=\"yolov8n.pt\")\n",
    "        print(f\"Detected {len(holds_detections)} hold instances across frames\")\n",
    "        \n",
    "        # Cluster detections into unique holds\n",
    "        clustered_holds = cluster_holds(holds_detections, eps=0.03, min_samples=2)\n",
    "        print(f\"Clustered into {len(clustered_holds)} unique holds\")\n",
    "        \n",
    "        if clustered_holds:\n",
    "            # Save to JSON\n",
    "            holds_json_path = export_holds_json(\n",
    "                clustered_holds, \n",
    "                output_path=FRAME_DIR / \"holds.json\"\n",
    "            )\n",
    "            print(f\"Exported to: {holds_json_path}\")\n",
    "            \n",
    "            # Display sample holds\n",
    "            print(\"\\nSample holds:\")\n",
    "            for h in clustered_holds[:3]:\n",
    "                print(f\"  {h.hold_id}: {h.label} at ({h.x:.3f}, {h.y:.3f})\")\n",
    "                print(f\"    confidence={h.avg_confidence:.2f}, detections={h.detections}\")\n",
    "    else:\n",
    "        print(\"Warning: No frame images found\")\n",
    "        clustered_holds = []\n",
    "else:\n",
    "    print(\"Warning: YOLO not available - skipping hold detection\")\n",
    "    clustered_holds = []\n",
    "\n",
    "# ----------------------------------------------------------------------------\n",
    "# STEP 2: Wall Angle Estimation\n",
    "# ----------------------------------------------------------------------------\n",
    "print(\"\\n[Step 2/5] Wall Angle Estimation\")\n",
    "print(\"-\" * 60)\n",
    "\n",
    "first_image = next(FRAME_DIR.glob(\"*.jpg\"), None)\n",
    "if first_image:\n",
    "    # Use hold centers for PCA refinement if available\n",
    "    hold_centers = [(h.x, h.y) for h in clustered_holds] if clustered_holds else None\n",
    "    \n",
    "    # Estimate wall angle\n",
    "    wall_result = estimate_wall_angle(first_image, hold_centers=hold_centers)\n",
    "    \n",
    "    if wall_result.angle_degrees is not None:\n",
    "        print(f\"Estimated angle: {wall_result.angle_degrees:.1f} degrees\")\n",
    "        print(f\"Confidence: {wall_result.confidence:.2f}\")\n",
    "        print(f\"Method: {wall_result.method}\")\n",
    "        print(f\"Hough lines detected: {len(wall_result.hough_lines)}\")\n",
    "        if wall_result.pca_angle:\n",
    "            print(f\"PCA angle: {wall_result.pca_angle:.1f} degrees\")\n",
    "    else:\n",
    "        print(\"Failed to estimate wall angle\")\n",
    "else:\n",
    "    print(\"Warning: No images found for wall angle estimation\")\n",
    "    wall_result = None\n",
    "\n",
    "# ----------------------------------------------------------------------------\n",
    "# STEP 3: Load Pose Features\n",
    "# ----------------------------------------------------------------------------\n",
    "print(\"\\n[Step 3/5] Loading Pose Features\")\n",
    "print(\"-\" * 60)\n",
    "\n",
    "features_path = FRAME_DIR / \"pose_features.json\"\n",
    "if features_path.exists():\n",
    "    with open(features_path) as f:\n",
    "        feature_rows = json.load(f)\n",
    "    \n",
    "    print(f\"Loaded {len(feature_rows)} feature rows\")\n",
    "    \n",
    "    # Check for new features\n",
    "    if feature_rows:\n",
    "        sample = feature_rows[0]\n",
    "        print(f\"Total features per row: {len(sample)}\")\n",
    "        \n",
    "        new_features = [\n",
    "            k for k in sample.keys() \n",
    "            if k in ['wall_angle', 'hip_alignment_error', 'com_along_wall', 'com_perp_wall']\n",
    "        ]\n",
    "        if new_features:\n",
    "            print(f\"New wall features present: {', '.join(new_features)}\")\n",
    "else:\n",
    "    print(\"Warning: pose_features.json not found\")\n",
    "    print(\"Tip: Run feature export with auto_wall_angle=True\")\n",
    "    feature_rows = []\n",
    "\n",
    "# ----------------------------------------------------------------------------\n",
    "# STEP 4: Calculate Efficiency Score\n",
    "# ----------------------------------------------------------------------------\n",
    "print(\"\\n[Step 4/5] Efficiency Score Calculation\")\n",
    "print(\"-\" * 60)\n",
    "\n",
    "if feature_rows:\n",
    "    # Calculate efficiency using recent frames\n",
    "    eff_result = efficiency_from_frames(feature_rows, window=5)\n",
    "    \n",
    "    print(f\"Overall Efficiency Score: {eff_result.score:.3f}\")\n",
    "    print(\"\\nComponent Breakdown:\")\n",
    "    for comp, val in eff_result.components.items():\n",
    "        print(f\"  {comp:20s}: {val:.3f}\")\n",
    "else:\n",
    "    print(\"Warning: No features available for efficiency calculation\")\n",
    "    eff_result = None\n",
    "\n",
    "# ----------------------------------------------------------------------------\n",
    "# STEP 5: Next Hold Recommendations\n",
    "# ----------------------------------------------------------------------------\n",
    "print(\"\\n[Step 5/5] Next Hold Recommendations\")\n",
    "print(\"-\" * 60)\n",
    "\n",
    "if feature_rows and clustered_holds:\n",
    "    # Convert holds to dictionary format\n",
    "    holds_dicts = [h.as_dict() for h in clustered_holds]\n",
    "    \n",
    "    # Get recommendations based on current state\n",
    "    next_holds = suggest_next_holds(feature_rows[-1], holds_dicts, top_k=3)\n",
    "    \n",
    "    print(f\"Top {len(next_holds)} recommended holds:\")\n",
    "    for i, hold in enumerate(next_holds, 1):\n",
    "        coords = hold.get('coords', [])\n",
    "        print(f\"  {i}. {hold.get('hold_id')} ({hold.get('label')})\")\n",
    "        print(f\"     Position: ({coords[0]:.3f}, {coords[1]:.3f})\")\n",
    "        print(f\"     Confidence: {hold.get('avg_confidence', 0):.2f}\")\n",
    "else:\n",
    "    print(\"Warning: Missing features or holds for recommendations\")\n",
    "\n",
    "# ----------------------------------------------------------------------------\n",
    "# Summary\n",
    "# ----------------------------------------------------------------------------\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"Demo Complete\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "print(\"\\nSummary:\")\n",
    "print(f\"  Holds detected: {len(clustered_holds)}\")\n",
    "if wall_result and wall_result.angle_degrees:\n",
    "    print(f\"  Wall angle: {wall_result.angle_degrees:.1f} degrees ({wall_result.method})\")\n",
    "else:\n",
    "    print(\"  Wall angle: Not available\")\n",
    "if eff_result:\n",
    "    print(f\"  Efficiency score: {eff_result.score:.3f}\")\n",
    "else:\n",
    "    print(\"  Efficiency score: Not available\")\n",
    "\n",
    "print(\"\\nTo run full pipeline with new features:\")\n",
    "print(\"  Option 1: Use web UI - upload video (auto-runs all features)\")\n",
    "print(\"  Option 2: CLI - python scripts/generate_holds_and_features.py <manifest_path>\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "6156 (py3.10)",
   "language": "python",
   "name": "6156-capstone"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
